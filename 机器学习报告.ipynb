{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSets=pd.DataFrame(data.data,columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSets['label']=data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  label  \n",
       "0                  0.2654          0.4601                  0.11890      0  \n",
       "1                  0.1860          0.2750                  0.08902      0  \n",
       "2                  0.2430          0.3613                  0.08758      0  \n",
       "3                  0.2575          0.6638                  0.17300      0  \n",
       "4                  0.1625          0.2364                  0.07678      0  \n",
       "..                    ...             ...                      ...    ...  \n",
       "564                0.2216          0.2060                  0.07115      0  \n",
       "565                0.1628          0.2572                  0.06637      0  \n",
       "566                0.1418          0.2218                  0.07820      0  \n",
       "567                0.2650          0.4087                  0.12400      0  \n",
       "568                0.0000          0.2871                  0.07039      1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  label                    569 non-null    int32  \n",
      "dtypes: float64(30), int32(1)\n",
      "memory usage: 135.7 KB\n"
     ]
    }
   ],
   "source": [
    "dataSets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    357\n",
       "0    212\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSets['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x=dataSets.iloc[:,:-1]\n",
    "data_y=dataSets.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x=np.array(data_x)\n",
    "data_y=np.array(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集划分完毕，用网格搜索找到最优模型。然后评测各类算法的优点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import  KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [1,2,3,5,8,10,15,20,25,30,35,40]\n",
    "weights = ['uniform','distance']\n",
    "param_grid = [{'n_neighbors': n_neighbors, 'weights': weights}]\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=time.time()\n",
    "grid_search.fit(data_x,data_y)\n",
    "t2=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8672411441802979"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00204377, 0.00191994, 0.0021992 , 0.00198944, 0.00184369,\n",
       "        0.00174899, 0.00189743, 0.00179482, 0.00189466, 0.00178385,\n",
       "        0.00183368, 0.00189774, 0.00168939, 0.0019336 , 0.00194492,\n",
       "        0.00170863, 0.00179601, 0.00209277, 0.00169263, 0.00183415,\n",
       "        0.00199621, 0.00178216, 0.0021277 , 0.00179796]),\n",
       " 'std_fit_time': array([3.50550750e-04, 2.12902339e-04, 5.86798744e-04, 1.61108345e-05,\n",
       "        3.35085536e-04, 3.96754360e-04, 2.98575865e-04, 3.98934004e-04,\n",
       "        2.99279043e-04, 4.52058463e-04, 3.51297410e-04, 5.32415817e-04,\n",
       "        5.06958074e-04, 1.89859092e-04, 1.38372902e-04, 4.67025426e-04,\n",
       "        3.99630036e-04, 2.87637941e-04, 4.61627543e-04, 3.31186818e-04,\n",
       "        4.39437420e-04, 3.94713006e-04, 2.53593747e-04, 4.06858561e-04]),\n",
       " 'mean_score_time': array([0.00204334, 0.00110323, 0.00189285, 0.00094824, 0.00189247,\n",
       "        0.00089478, 0.00179718, 0.00099723, 0.00194848, 0.00060401,\n",
       "        0.00198519, 0.00085342, 0.00175982, 0.00105786, 0.0020942 ,\n",
       "        0.00098383, 0.00197222, 0.00130775, 0.0022001 , 0.00120485,\n",
       "        0.00214663, 0.00110357, 0.00196071, 0.00109956]),\n",
       " 'std_score_time': array([5.66704765e-04, 2.92500519e-04, 2.85927477e-04, 3.40970275e-04,\n",
       "        2.98779409e-04, 2.98433346e-04, 4.08897809e-04, 3.38016674e-07,\n",
       "        3.55743534e-04, 4.93284551e-04, 1.28610195e-05, 4.57428674e-04,\n",
       "        5.10689366e-04, 1.90374600e-04, 1.93593245e-04, 2.47430360e-05,\n",
       "        4.01419825e-04, 4.64633213e-04, 5.96661646e-04, 4.02262993e-04,\n",
       "        3.23979816e-04, 3.43315420e-04, 3.59896881e-04, 3.07170645e-04]),\n",
       " 'param_n_neighbors': masked_array(data=[1, 1, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 15, 15, 20, 20,\n",
       "                    25, 25, 30, 30, 35, 35, 40, 40],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_weights': masked_array(data=['uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'n_neighbors': 1, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 1, 'weights': 'distance'},\n",
       "  {'n_neighbors': 2, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 2, 'weights': 'distance'},\n",
       "  {'n_neighbors': 3, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 3, 'weights': 'distance'},\n",
       "  {'n_neighbors': 5, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 5, 'weights': 'distance'},\n",
       "  {'n_neighbors': 8, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 8, 'weights': 'distance'},\n",
       "  {'n_neighbors': 10, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 10, 'weights': 'distance'},\n",
       "  {'n_neighbors': 15, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 15, 'weights': 'distance'},\n",
       "  {'n_neighbors': 20, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 20, 'weights': 'distance'},\n",
       "  {'n_neighbors': 25, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 25, 'weights': 'distance'},\n",
       "  {'n_neighbors': 30, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 30, 'weights': 'distance'},\n",
       "  {'n_neighbors': 35, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 35, 'weights': 'distance'},\n",
       "  {'n_neighbors': 40, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 40, 'weights': 'distance'}],\n",
       " 'split0_test_score': array([0.92982456, 0.92982456, 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.9122807 , 0.9122807 , 0.9122807 , 0.92982456, 0.92982456,\n",
       "        0.89473684, 0.9122807 , 0.87719298, 0.87719298, 0.87719298,\n",
       "        0.87719298, 0.87719298, 0.87719298, 0.87719298, 0.87719298,\n",
       "        0.87719298, 0.87719298, 0.87719298, 0.87719298]),\n",
       " 'split1_test_score': array([0.84210526, 0.84210526, 0.9122807 , 0.84210526, 0.85964912,\n",
       "        0.85964912, 0.87719298, 0.87719298, 0.85964912, 0.84210526,\n",
       "        0.85964912, 0.84210526, 0.85964912, 0.85964912, 0.85964912,\n",
       "        0.85964912, 0.84210526, 0.84210526, 0.84210526, 0.84210526,\n",
       "        0.85964912, 0.84210526, 0.84210526, 0.84210526]),\n",
       " 'split2_test_score': array([0.92982456, 0.92982456, 0.89473684, 0.92982456, 0.89473684,\n",
       "        0.9122807 , 0.89473684, 0.9122807 , 0.9122807 , 0.9122807 ,\n",
       "        0.9122807 , 0.9122807 , 0.9122807 , 0.9122807 , 0.9122807 ,\n",
       "        0.9122807 , 0.9122807 , 0.9122807 , 0.89473684, 0.9122807 ,\n",
       "        0.87719298, 0.89473684, 0.87719298, 0.89473684]),\n",
       " 'split3_test_score': array([0.92982456, 0.92982456, 0.92982456, 0.92982456, 0.94736842,\n",
       "        0.94736842, 0.96491228, 0.96491228, 0.96491228, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.96491228, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.96491228, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.94736842, 0.96491228]),\n",
       " 'split4_test_score': array([0.9122807 , 0.9122807 , 0.92982456, 0.9122807 , 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.96491228, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.96491228, 0.92982456, 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.92982456]),\n",
       " 'split5_test_score': array([0.89473684, 0.89473684, 0.92982456, 0.89473684, 0.94736842,\n",
       "        0.94736842, 0.92982456, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.92982456, 0.94736842,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.92982456]),\n",
       " 'split6_test_score': array([0.92982456, 0.92982456, 0.9122807 , 0.92982456, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.96491228, 0.96491228,\n",
       "        0.96491228, 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.94736842]),\n",
       " 'split7_test_score': array([0.94736842, 0.94736842, 0.89473684, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.92982456, 0.92982456, 0.92982456, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.94736842]),\n",
       " 'split8_test_score': array([0.89473684, 0.89473684, 0.87719298, 0.89473684, 0.9122807 ,\n",
       "        0.92982456, 0.9122807 , 0.9122807 , 0.89473684, 0.9122807 ,\n",
       "        0.9122807 , 0.9122807 , 0.9122807 , 0.9122807 , 0.92982456,\n",
       "        0.9122807 , 0.94736842, 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.92982456]),\n",
       " 'split9_test_score': array([0.94642857, 0.94642857, 0.92857143, 0.94642857, 0.92857143,\n",
       "        0.92857143, 0.96428571, 0.96428571, 0.96428571, 0.96428571,\n",
       "        0.96428571, 0.96428571, 0.96428571, 0.94642857, 0.94642857,\n",
       "        0.96428571, 0.96428571, 0.98214286, 0.96428571, 0.96428571,\n",
       "        0.92857143, 0.96428571, 0.92857143, 0.94642857]),\n",
       " 'mean_test_score': array([0.91569549, 0.91569549, 0.91215539, 0.91569549, 0.92619048,\n",
       "        0.92969925, 0.9297619 , 0.9297619 , 0.9297619 , 0.9297619 ,\n",
       "        0.93151629, 0.93151629, 0.9297619 , 0.92797619, 0.92973058,\n",
       "        0.93151629, 0.92800752, 0.92979323, 0.92274436, 0.92449875,\n",
       "        0.92092732, 0.92449875, 0.91741855, 0.92095865]),\n",
       " 'std_test_score': array([0.03008643, 0.03008643, 0.01742215, 0.03008643, 0.03017137,\n",
       "        0.02829112, 0.02928211, 0.02593781, 0.03228179, 0.03413545,\n",
       "        0.034542  , 0.03711902, 0.036741  , 0.03543023, 0.0341522 ,\n",
       "        0.03711902, 0.03793336, 0.04147672, 0.03772122, 0.03760156,\n",
       "        0.03439365, 0.03841138, 0.03595618, 0.03610025]),\n",
       " 'rank_test_score': array([21, 21, 24, 21, 14, 11,  6,  6,  5,  6,  1,  1,  6, 13, 10,  1, 12,\n",
       "         4, 17, 15, 19, 15, 20, 18])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9315162907268169,\n",
       " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                      metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                      weights='uniform'),\n",
       " {'n_neighbors': 10, 'weights': 'uniform'})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_, grid_search.best_estimator_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty=['l1','l2']\n",
    "C = [0.1, 0.5, 1, 5, 10]\n",
    "param_grid=[{'penalty':penalty,'C':C}]\n",
    "grid_search=GridSearchCV(clf,param_grid=param_grid,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "grid_search.fit(data_x,data_y)\n",
    "t2=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992866516113281"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.        , 0.01895807, 0.        , 0.01979117, 0.        ,\n",
       "        0.01864839, 0.        , 0.01779447, 0.        , 0.01824777]),\n",
       " 'std_fit_time': array([0.        , 0.00314003, 0.        , 0.00223821, 0.        ,\n",
       "        0.00141323, 0.        , 0.00055552, 0.        , 0.00154983]),\n",
       " 'mean_score_time': array([0.        , 0.00039892, 0.        , 0.0004986 , 0.        ,\n",
       "        0.00019941, 0.        , 0.00015628, 0.        , 0.00029972]),\n",
       " 'std_score_time': array([0.        , 0.00048858, 0.        , 0.0004986 , 0.        ,\n",
       "        0.00039883, 0.        , 0.00032714, 0.        , 0.00045785]),\n",
       " 'param_C': masked_array(data=[0.1, 0.1, 0.5, 0.5, 1, 1, 5, 5, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.1, 'penalty': 'l1'},\n",
       "  {'C': 0.1, 'penalty': 'l2'},\n",
       "  {'C': 0.5, 'penalty': 'l1'},\n",
       "  {'C': 0.5, 'penalty': 'l2'},\n",
       "  {'C': 1, 'penalty': 'l1'},\n",
       "  {'C': 1, 'penalty': 'l2'},\n",
       "  {'C': 5, 'penalty': 'l1'},\n",
       "  {'C': 5, 'penalty': 'l2'},\n",
       "  {'C': 10, 'penalty': 'l1'},\n",
       "  {'C': 10, 'penalty': 'l2'}],\n",
       " 'split0_test_score': array([       nan, 0.92982456,        nan, 0.92982456,        nan,\n",
       "        0.92982456,        nan, 0.92982456,        nan, 0.94736842]),\n",
       " 'split1_test_score': array([       nan, 0.92982456,        nan, 0.92982456,        nan,\n",
       "        0.92982456,        nan, 0.92982456,        nan, 0.92982456]),\n",
       " 'split2_test_score': array([       nan, 0.94736842,        nan, 0.94736842,        nan,\n",
       "        0.92982456,        nan, 0.94736842,        nan, 0.94736842]),\n",
       " 'split3_test_score': array([       nan, 0.92982456,        nan, 0.92982456,        nan,\n",
       "        0.92982456,        nan, 0.92982456,        nan, 0.92982456]),\n",
       " 'split4_test_score': array([       nan, 0.98245614,        nan, 0.96491228,        nan,\n",
       "        0.94736842,        nan, 0.94736842,        nan, 0.96491228]),\n",
       " 'split5_test_score': array([       nan, 0.96491228,        nan, 0.98245614,        nan,\n",
       "        0.96491228,        nan, 0.96491228,        nan, 0.94736842]),\n",
       " 'split6_test_score': array([       nan, 0.92982456,        nan, 0.92982456,        nan,\n",
       "        0.92982456,        nan, 0.92982456,        nan, 0.94736842]),\n",
       " 'split7_test_score': array([       nan, 0.89473684,        nan, 0.9122807 ,        nan,\n",
       "        0.94736842,        nan, 0.96491228,        nan, 0.92982456]),\n",
       " 'split8_test_score': array([       nan, 0.9122807 ,        nan, 0.96491228,        nan,\n",
       "        0.96491228,        nan, 0.94736842,        nan, 0.92982456]),\n",
       " 'split9_test_score': array([       nan, 0.96428571,        nan, 0.96428571,        nan,\n",
       "        0.96428571,        nan, 0.96428571,        nan, 0.96428571]),\n",
       " 'mean_test_score': array([       nan, 0.93853383,        nan, 0.94555138,        nan,\n",
       "        0.94379699,        nan, 0.94555138,        nan, 0.94379699]),\n",
       " 'std_test_score': array([       nan, 0.02505396,        nan, 0.0213593 ,        nan,\n",
       "        0.01520905,        nan, 0.01449103,        nan, 0.01302912]),\n",
       " 'rank_test_score': array([ 6,  5,  7,  1,  8,  3,  9,  1, 10,  3])}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9455513784461151,\n",
       " LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " {'C': 0.5, 'penalty': 'l2'})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_, grid_search.best_estimator_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=time.time()\n",
    "scores=cross_val_score(clf,data_x,data_y,cv=10,scoring='accuracy')\n",
    "t2=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010969161987304688"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94736842, 0.87719298, 0.89473684, 0.92982456, 0.94736842,\n",
       "       0.96491228, 0.92982456, 0.96491228, 0.94736842, 0.96428571])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ['gini','entropy']\n",
    "splitter=['best','random']\n",
    "max_depth = [10, 15, 20, 30, None]\n",
    "min_samples_split = [2, 3, 5, 8, 10]\n",
    "min_samples_leaf = [1, 2, 3, 5, 8]\n",
    "param_grid = [{'criterion': criterion,'splitter':splitter, 'max_depth':max_depth, 'min_samples_split':min_samples_split, 'min_samples_leaf':min_samples_leaf}]\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=time.time()\n",
    "grid_search.fit(data_x,data_y)\n",
    "t2=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.825528383255005"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([4.84158993e-03, 5.98239899e-04, 4.76648808e-03, 7.10320473e-04,\n",
       "        5.20756245e-03, 8.97431374e-04, 4.89449501e-03, 6.59060478e-04,\n",
       "        5.08718491e-03, 9.63282585e-04, 4.42318916e-03, 6.98065758e-04,\n",
       "        4.55255508e-03, 6.98709488e-04, 4.18791771e-03, 7.98273087e-04,\n",
       "        4.57675457e-03, 6.48665428e-04, 4.68835831e-03, 7.59863853e-04,\n",
       "        4.38885689e-03, 5.97929955e-04, 4.48286533e-03, 6.97875023e-04,\n",
       "        4.09054756e-03, 4.98533249e-04, 4.68688011e-03, 5.92970848e-04,\n",
       "        4.28209305e-03, 4.98962402e-04, 4.38458920e-03, 4.00185585e-04,\n",
       "        4.48994637e-03, 6.18052483e-04, 4.38466072e-03, 7.00736046e-04,\n",
       "        4.15718555e-03, 6.95443153e-04, 4.19023037e-03, 5.98120689e-04,\n",
       "        4.22966480e-03, 4.94766235e-04, 3.93731594e-03, 8.51297379e-04,\n",
       "        4.18353081e-03, 3.98445129e-04, 3.77271175e-03, 8.97598267e-04,\n",
       "        4.02405262e-03, 3.22651863e-04, 4.83429432e-03, 7.30085373e-04,\n",
       "        4.98287678e-03, 7.97653198e-04, 4.78515625e-03, 6.98947906e-04,\n",
       "        4.81250286e-03, 6.32333755e-04, 4.78906631e-03, 7.99179077e-04,\n",
       "        4.40216064e-03, 8.97526741e-04, 4.34236526e-03, 7.96651840e-04,\n",
       "        4.45911884e-03, 6.60276413e-04, 4.39646244e-03, 8.98742676e-04,\n",
       "        4.59294319e-03, 9.00340080e-04, 5.01554012e-03, 8.44192505e-04,\n",
       "        5.08499146e-03, 8.91780853e-04, 5.07602692e-03, 8.05664062e-04,\n",
       "        4.51393127e-03, 6.04748726e-04, 4.58583832e-03, 6.98208809e-04,\n",
       "        4.48386669e-03, 7.24625587e-04, 4.37450409e-03, 5.99861145e-04,\n",
       "        3.98821831e-03, 5.57661057e-04, 4.10006046e-03, 7.61532784e-04,\n",
       "        4.19073105e-03, 8.22138786e-04, 4.09047604e-03, 3.00478935e-04,\n",
       "        4.25641537e-03, 8.41379166e-04, 4.06346321e-03, 4.98914719e-04,\n",
       "        4.32147980e-03, 4.98437881e-04, 4.13193703e-03, 4.95481491e-04,\n",
       "        5.22320271e-03, 7.98368454e-04, 4.84020710e-03, 9.41801071e-04,\n",
       "        4.46527004e-03, 5.35535812e-04, 4.62152958e-03, 6.35480881e-04,\n",
       "        4.88691330e-03, 8.90779495e-04, 5.01158237e-03, 6.99257851e-04,\n",
       "        4.78374958e-03, 7.98225403e-04, 4.31811810e-03, 7.07435608e-04,\n",
       "        4.64477539e-03, 5.98335266e-04, 4.18889523e-03, 4.98867035e-04,\n",
       "        4.38766479e-03, 4.98008728e-04, 4.47769165e-03, 5.98406792e-04,\n",
       "        4.51922417e-03, 8.42714310e-04, 4.58226204e-03, 8.64267349e-04,\n",
       "        4.58903313e-03, 6.45065308e-04, 3.95383835e-03, 7.97796249e-04,\n",
       "        4.14485931e-03, 5.98335266e-04, 4.12728786e-03, 6.46758080e-04,\n",
       "        4.23929691e-03, 9.00387764e-04, 4.12890911e-03, 4.57954407e-04,\n",
       "        4.28519249e-03, 3.98755074e-04, 3.98354530e-03, 3.02124023e-04,\n",
       "        4.19285297e-03, 6.97994232e-04, 4.42502499e-03, 6.95157051e-04,\n",
       "        3.77259254e-03, 9.68933105e-05, 4.84638214e-03, 7.94553757e-04,\n",
       "        5.02483845e-03, 7.55572319e-04, 5.20095825e-03, 6.94894791e-04,\n",
       "        4.78610992e-03, 5.98073006e-04, 4.79865074e-03, 8.56089592e-04,\n",
       "        4.66661453e-03, 1.05762482e-03, 4.50055599e-03, 9.09543037e-04,\n",
       "        4.71165180e-03, 7.34972954e-04, 4.54676151e-03, 9.95516777e-04,\n",
       "        4.45706844e-03, 7.92074203e-04, 4.45351601e-03, 7.57884979e-04,\n",
       "        4.82058525e-03, 8.95118713e-04, 4.59032059e-03, 7.97772408e-04,\n",
       "        4.51819897e-03, 8.56447220e-04, 4.11167145e-03, 7.94863701e-04,\n",
       "        4.65977192e-03, 7.00545311e-04, 3.97000313e-03, 5.51199913e-04,\n",
       "        4.23922539e-03, 5.61451912e-04, 4.26721573e-03, 8.67247581e-04,\n",
       "        4.23364639e-03, 5.97548485e-04, 4.20258045e-03, 6.12473488e-04,\n",
       "        4.02128696e-03, 5.04708290e-04, 4.61885929e-03, 8.81123543e-04,\n",
       "        4.11968231e-03, 9.21320915e-04, 4.21259403e-03, 2.90036201e-04,\n",
       "        5.08966446e-03, 6.96015358e-04, 5.33642769e-03, 7.55405426e-04,\n",
       "        5.16612530e-03, 9.63830948e-04, 5.05435467e-03, 7.36999512e-04,\n",
       "        4.69281673e-03, 5.61738014e-04, 4.31101322e-03, 7.01093674e-04,\n",
       "        4.38840389e-03, 7.77387619e-04, 4.39357758e-03, 7.00306892e-04,\n",
       "        4.86402512e-03, 8.00466537e-04, 4.03823853e-03, 9.56106186e-04,\n",
       "        4.58858013e-03, 8.72159004e-04, 4.57823277e-03, 6.98065758e-04,\n",
       "        4.29387093e-03, 9.00387764e-04, 4.18918133e-03, 9.97209549e-04,\n",
       "        4.49090004e-03, 5.95426559e-04, 4.17454243e-03, 7.00473785e-04,\n",
       "        3.98344994e-03, 7.98892975e-04, 4.67867851e-03, 8.99386406e-04,\n",
       "        4.12209034e-03, 6.01553917e-04, 3.99641991e-03, 9.41395760e-04,\n",
       "        3.75287533e-03, 3.98612022e-04, 3.98907661e-03, 9.99832153e-04,\n",
       "        3.87098789e-03, 4.98485565e-04, 4.04005051e-03, 5.95521927e-04,\n",
       "        4.05921936e-03, 3.98159027e-04, 5.97019196e-03, 8.27503204e-04,\n",
       "        6.07953072e-03, 8.07952881e-04, 5.98535538e-03, 5.40232658e-04,\n",
       "        5.97524643e-03, 5.01227379e-04, 6.06486797e-03, 3.98945808e-04,\n",
       "        5.66449165e-03, 5.95569611e-04, 5.88636398e-03, 5.54442406e-04,\n",
       "        5.58440685e-03, 6.97875023e-04, 5.49294949e-03, 7.33470917e-04,\n",
       "        5.86314201e-03, 6.00743294e-04, 5.68671227e-03, 6.58297539e-04,\n",
       "        5.83152771e-03, 6.98089600e-04, 5.81011772e-03, 6.00910187e-04,\n",
       "        5.88617325e-03, 6.01100922e-04, 5.79295158e-03, 7.94744492e-04,\n",
       "        5.99226952e-03, 9.98449326e-04, 5.96957207e-03, 3.50332260e-04,\n",
       "        5.88912964e-03, 8.00490379e-04, 5.69090843e-03, 4.56380844e-04,\n",
       "        5.72891235e-03, 2.99096107e-04, 5.18770218e-03, 6.01959229e-04,\n",
       "        5.12835979e-03, 7.94863701e-04, 5.18033504e-03, 9.93871689e-04,\n",
       "        5.64758778e-03, 1.99341774e-04, 5.18326759e-03, 3.98921967e-04,\n",
       "        6.09250069e-03, 5.96714020e-04, 5.82892895e-03, 8.97145271e-04,\n",
       "        6.14411831e-03, 7.43746758e-04, 5.57963848e-03, 7.98273087e-04,\n",
       "        5.52804470e-03, 1.06647015e-03, 6.28759861e-03, 5.98287582e-04,\n",
       "        5.93705177e-03, 9.60278511e-04, 5.58471680e-03, 7.97200203e-04,\n",
       "        5.76195717e-03, 5.50055504e-04, 5.48193455e-03, 5.95474243e-04,\n",
       "        5.50980568e-03, 7.41124153e-04, 5.69224358e-03, 5.02014160e-04,\n",
       "        6.06627464e-03, 6.01410866e-04, 5.86857796e-03, 5.97524643e-04,\n",
       "        5.89256287e-03, 7.97414780e-04, 5.73465824e-03, 5.99050522e-04,\n",
       "        5.83715439e-03, 5.98096848e-04, 5.48436642e-03, 2.99096107e-04,\n",
       "        5.67798615e-03, 8.06188583e-04, 5.86376190e-03, 7.95149803e-04,\n",
       "        5.19053936e-03, 7.59649277e-04, 5.37776947e-03, 4.93407249e-04,\n",
       "        5.48791885e-03, 6.70957565e-04, 5.38530350e-03, 8.62240791e-04,\n",
       "        5.45496941e-03, 5.99193573e-04, 5.79483509e-03, 9.53793526e-04,\n",
       "        5.78126907e-03, 8.95571709e-04, 5.93659878e-03, 7.09199905e-04,\n",
       "        5.68897724e-03, 6.08038902e-04, 5.76837063e-03, 6.94942474e-04,\n",
       "        5.73892593e-03, 8.48436356e-04, 5.83932400e-03, 8.86869431e-04,\n",
       "        5.91187477e-03, 8.11648369e-04, 5.73308468e-03, 6.40535355e-04,\n",
       "        5.76696396e-03, 6.95514679e-04, 5.88064194e-03, 7.95054436e-04,\n",
       "        6.19764328e-03, 7.97724724e-04, 5.64215183e-03, 7.98082352e-04,\n",
       "        5.85029125e-03, 4.46081161e-04, 5.70499897e-03, 8.00633430e-04,\n",
       "        5.41751385e-03, 7.94839859e-04, 5.33978939e-03, 9.97233391e-04,\n",
       "        5.98530769e-03, 9.53841209e-04, 5.69903851e-03, 3.47280502e-04,\n",
       "        5.58094978e-03, 7.95578957e-04, 5.44521809e-03, 4.99343872e-04,\n",
       "        5.00576496e-03, 3.38768959e-04, 5.06925583e-03, 9.97281075e-04,\n",
       "        5.38716316e-03, 5.39541245e-05, 5.40883541e-03, 5.98120689e-04,\n",
       "        6.22894764e-03, 9.74607468e-04, 5.80699444e-03, 7.96937943e-04,\n",
       "        6.09095097e-03, 1.10385418e-03, 6.01685047e-03, 5.66172600e-04,\n",
       "        6.02447987e-03, 8.03327560e-04, 6.28960133e-03, 5.54966927e-04,\n",
       "        5.62231541e-03, 8.59332085e-04, 5.64787388e-03, 5.96046448e-04,\n",
       "        5.68661690e-03, 7.00855255e-04, 5.95779419e-03, 8.00442696e-04,\n",
       "        5.90672493e-03, 8.97359848e-04, 5.70633411e-03, 7.00974464e-04,\n",
       "        5.75270653e-03, 5.95498085e-04, 5.95242977e-03, 7.14874268e-04,\n",
       "        6.22966290e-03, 7.63320923e-04, 5.88352680e-03, 9.04273987e-04,\n",
       "        5.69257736e-03, 7.43007660e-04, 5.88855743e-03, 6.98113441e-04,\n",
       "        5.82919121e-03, 7.94935226e-04, 6.18882179e-03, 7.56478310e-04,\n",
       "        5.29046059e-03, 1.00014210e-03, 5.24933338e-03, 0.00000000e+00,\n",
       "        4.91232872e-03, 6.01553917e-04, 5.56979179e-03, 6.01220131e-04,\n",
       "        5.20055294e-03, 2.47240067e-04, 5.73358536e-03, 6.50596619e-04,\n",
       "        5.88448048e-03, 6.98328018e-04, 5.94129562e-03, 6.98018074e-04,\n",
       "        5.46333790e-03, 8.68248940e-04, 6.06300831e-03, 5.98645210e-04,\n",
       "        5.81157207e-03, 8.00514221e-04, 5.94623089e-03, 9.06872749e-04,\n",
       "        5.56614399e-03, 5.98883629e-04, 6.18293285e-03, 6.67548180e-04,\n",
       "        5.39002419e-03, 5.98311424e-04, 5.92017174e-03, 9.55891609e-04,\n",
       "        5.82675934e-03, 9.50765610e-04, 5.68122864e-03, 9.00506973e-04,\n",
       "        5.66813946e-03, 8.23545456e-04, 5.69763184e-03, 8.97526741e-04,\n",
       "        5.58722019e-03, 8.97741318e-04, 5.59587479e-03, 8.97431374e-04,\n",
       "        5.71353436e-03, 1.00111961e-04, 5.82334995e-03, 5.16939163e-04,\n",
       "        5.78336716e-03, 9.01269913e-04, 5.61885834e-03, 7.84349442e-04,\n",
       "        5.36308289e-03, 7.94935226e-04, 4.85391617e-03, 6.01124763e-04,\n",
       "        5.11221886e-03, 7.00807571e-04, 4.99200821e-03, 5.09023666e-05]),\n",
       " 'std_fit_time': array([5.45441293e-04, 5.26109790e-04, 3.90322081e-04, 4.70268231e-04,\n",
       "        5.04890649e-04, 2.99431648e-04, 5.33691125e-04, 5.66013812e-04,\n",
       "        5.45226675e-04, 3.72217694e-04, 4.74220029e-04, 4.57146428e-04,\n",
       "        4.78125819e-04, 4.57680146e-04, 6.03223084e-04, 3.99138947e-04,\n",
       "        4.80774739e-04, 4.48664886e-04, 4.57883514e-04, 3.96250595e-04,\n",
       "        5.05347397e-04, 4.88319217e-04, 5.11387736e-04, 4.57036557e-04,\n",
       "        3.00634994e-04, 4.98804831e-04, 4.56542679e-04, 4.84266454e-04,\n",
       "        4.61690474e-04, 4.99299144e-04, 6.61400177e-04, 4.90323366e-04,\n",
       "        5.02101544e-04, 4.72640140e-04, 4.89530792e-04, 4.59058297e-04,\n",
       "        3.51896603e-04, 4.55379641e-04, 5.99500286e-04, 4.88532655e-04,\n",
       "        4.09314393e-04, 4.94928463e-04, 6.81651569e-04, 3.06903434e-04,\n",
       "        3.96152239e-04, 5.12140043e-04, 3.99801491e-04, 2.99471688e-04,\n",
       "        4.57413428e-04, 4.42363338e-04, 5.55504855e-04, 3.89634359e-04,\n",
       "        6.30100152e-04, 5.96752694e-04, 3.98763398e-04, 4.57981726e-04,\n",
       "        3.20009986e-04, 4.52706481e-04, 3.94430503e-04, 3.99655340e-04,\n",
       "        4.99150554e-04, 2.99427809e-04, 4.41372291e-04, 3.98834481e-04,\n",
       "        4.32754212e-04, 4.47009610e-04, 6.57066893e-04, 2.99746096e-04,\n",
       "        4.84106113e-04, 3.00207224e-04, 5.36425506e-04, 4.47730830e-04,\n",
       "        5.43160705e-04, 2.97525299e-04, 3.25928509e-04, 4.03504731e-04,\n",
       "        4.87144491e-04, 4.94003851e-04, 4.95386330e-04, 4.57211154e-04,\n",
       "        4.90035876e-04, 3.87410707e-04, 4.69368790e-04, 4.89805460e-04,\n",
       "        1.31605328e-05, 4.65736192e-04, 2.12849539e-04, 3.99059551e-04,\n",
       "        4.04653082e-04, 4.18191789e-04, 5.32245126e-04, 4.59002022e-04,\n",
       "        4.07793539e-04, 3.21764771e-04, 3.35516297e-04, 4.98924670e-04,\n",
       "        4.40699924e-04, 4.98438456e-04, 4.65198226e-04, 4.95720685e-04,\n",
       "        6.90564777e-04, 3.99464921e-04, 4.50500218e-04, 3.42730482e-04,\n",
       "        5.25465895e-04, 4.76269763e-04, 5.58388594e-04, 4.50668977e-04,\n",
       "        6.99152886e-04, 2.97240523e-04, 4.67372658e-04, 4.57798868e-04,\n",
       "        3.98890062e-04, 5.98552300e-04, 4.66610831e-04, 4.45509340e-04,\n",
       "        6.30569710e-04, 4.88538777e-04, 3.98707202e-04, 4.99038213e-04,\n",
       "        5.00955787e-04, 4.98168964e-04, 5.12862424e-04, 4.88945291e-04,\n",
       "        5.37036249e-04, 3.30600786e-04, 4.96147885e-04, 4.70236984e-04,\n",
       "        4.79935306e-04, 5.44329439e-04, 5.17671154e-04, 3.99513209e-04,\n",
       "        5.55714306e-04, 5.15006562e-04, 2.94321791e-04, 4.52750705e-04,\n",
       "        4.02725404e-04, 3.00261736e-04, 4.25336052e-04, 4.70468910e-04,\n",
       "        4.71546725e-04, 4.88378694e-04, 4.46466731e-04, 4.61549155e-04,\n",
       "        3.90015974e-04, 4.56945403e-04, 6.49533300e-04, 4.55165009e-04,\n",
       "        4.38636891e-04, 2.90679932e-04, 5.42587886e-04, 3.97367650e-04,\n",
       "        2.35368387e-04, 5.19009554e-04, 4.50372976e-04, 4.55152960e-04,\n",
       "        3.83142191e-04, 4.88436177e-04, 5.98651296e-04, 4.59898069e-04,\n",
       "        5.93353413e-04, 4.86829687e-04, 5.29295679e-04, 3.04678263e-04,\n",
       "        5.82469134e-04, 4.95356930e-04, 4.79362337e-04, 1.82461880e-05,\n",
       "        6.71299398e-04, 3.96375671e-04, 4.61670282e-04, 4.11487202e-04,\n",
       "        3.00829474e-04, 2.98768394e-04, 4.98671387e-04, 3.98894454e-04,\n",
       "        4.64932271e-04, 3.09824238e-04, 2.93627173e-04, 3.97772379e-04,\n",
       "        4.47278938e-04, 4.58812159e-04, 6.75771124e-05, 5.05851497e-04,\n",
       "        5.96179901e-04, 4.79534643e-04, 4.28269071e-04, 3.05849673e-04,\n",
       "        4.44756805e-04, 4.87993142e-04, 4.05558910e-04, 4.78781497e-04,\n",
       "        6.18858135e-04, 5.04775877e-04, 4.69574086e-04, 3.00380918e-04,\n",
       "        5.41859846e-04, 3.14121035e-04, 7.93016733e-04, 4.00352408e-04,\n",
       "        4.87717453e-04, 4.55880277e-04, 4.40307055e-04, 4.01201476e-04,\n",
       "        5.50485194e-04, 3.76732457e-04, 5.53354363e-04, 4.06496179e-04,\n",
       "        4.53966911e-04, 4.73510312e-04, 5.33771598e-04, 4.59623386e-04,\n",
       "        4.89218507e-04, 5.63463211e-04, 4.77834326e-04, 4.58760952e-04,\n",
       "        5.32951474e-04, 4.00343604e-04, 2.51005314e-04, 3.62689107e-04,\n",
       "        4.33728134e-04, 3.28309317e-04, 4.80498127e-04, 4.57360200e-04,\n",
       "        4.46320877e-04, 3.00527848e-04, 3.98591729e-04, 4.47299525e-04,\n",
       "        5.00750080e-04, 4.86416969e-04, 3.72451992e-04, 4.58636817e-04,\n",
       "        4.51649559e-04, 3.99551367e-04, 4.52217209e-04, 2.99929683e-04,\n",
       "        6.40935971e-04, 4.91375803e-04, 4.70682961e-05, 3.41549860e-04,\n",
       "        3.78553074e-04, 4.88380256e-04, 4.49025847e-04, 1.55141444e-05,\n",
       "        5.40709584e-04, 4.98650390e-04, 5.60409205e-04, 4.86470741e-04,\n",
       "        5.47532056e-04, 4.87644182e-04, 4.45731970e-04, 5.55651057e-04,\n",
       "        6.99077468e-04, 5.70238608e-04, 4.36071755e-04, 4.71843890e-04,\n",
       "        4.52042506e-04, 5.01430731e-04, 5.69251311e-04, 4.88826271e-04,\n",
       "        6.30437452e-04, 4.86343105e-04, 5.39525839e-04, 4.72747939e-04,\n",
       "        4.82711267e-04, 4.56867439e-04, 6.43301348e-04, 4.08941413e-04,\n",
       "        5.31601439e-04, 4.90893781e-04, 6.27634333e-04, 4.43592275e-04,\n",
       "        4.33710891e-04, 4.57184830e-04, 4.72838975e-04, 4.90926378e-04,\n",
       "        7.11070573e-04, 4.90873536e-04, 4.14584671e-04, 3.97476296e-04,\n",
       "        6.16434620e-04, 1.30119765e-05, 4.02699268e-04, 4.46712278e-04,\n",
       "        3.02377474e-04, 4.00661911e-04, 4.02789801e-04, 4.71643594e-04,\n",
       "        5.64970952e-04, 4.56877751e-04, 5.92488722e-04, 4.92523954e-04,\n",
       "        5.35157865e-04, 3.97784497e-04, 5.49054129e-04, 8.06700810e-06,\n",
       "        4.42555262e-04, 3.98683637e-04, 5.99144265e-04, 4.88577691e-04,\n",
       "        5.71371921e-04, 4.87231543e-04, 5.39547545e-04, 2.99489543e-04,\n",
       "        8.38426357e-04, 4.04034992e-04, 6.70068085e-04, 3.99378260e-04,\n",
       "        4.74685560e-04, 4.81905755e-04, 6.35748123e-04, 4.88693387e-04,\n",
       "        5.73038376e-04, 3.70888835e-04, 4.88177769e-04, 3.98601413e-04,\n",
       "        5.97485210e-04, 4.74047037e-04, 5.02577378e-04, 4.86457039e-04,\n",
       "        4.34007385e-04, 5.03914655e-04, 6.38172771e-04, 5.02144318e-04,\n",
       "        5.40896157e-04, 4.91108827e-04, 5.92418177e-04, 4.88186348e-04,\n",
       "        5.39080243e-04, 3.99457308e-04, 5.90061153e-04, 4.89445775e-04,\n",
       "        5.35740286e-04, 4.88524709e-04, 4.98648699e-04, 4.56877278e-04,\n",
       "        4.66024299e-04, 4.03489705e-04, 5.29747726e-04, 5.97333810e-04,\n",
       "        3.90292307e-04, 3.96277539e-04, 4.76984979e-04, 4.93503149e-04,\n",
       "        8.00141419e-04, 4.47691801e-04, 4.62047926e-04, 3.03913319e-04,\n",
       "        4.67722138e-04, 4.89273913e-04, 7.54172387e-04, 3.64230702e-04,\n",
       "        6.02230797e-04, 2.98569561e-04, 5.67683633e-04, 4.35365319e-04,\n",
       "        4.62348647e-04, 4.97880826e-04, 3.92403550e-04, 4.55188621e-04,\n",
       "        4.17710940e-04, 4.49975883e-04, 6.38800158e-04, 3.02508436e-04,\n",
       "        4.96532765e-04, 4.10340388e-04, 4.01150635e-04, 4.37038817e-04,\n",
       "        6.16259076e-04, 4.55793145e-04, 3.00367780e-04, 3.98116467e-04,\n",
       "        5.88869293e-04, 3.99076644e-04, 6.12244552e-04, 4.20434358e-04,\n",
       "        7.28497068e-04, 5.61030813e-04, 3.90826222e-04, 4.00708815e-04,\n",
       "        4.78861411e-04, 3.97514315e-04, 6.26302954e-04, 1.81097862e-05,\n",
       "        7.78955392e-04, 3.60594588e-04, 4.72823287e-04, 4.48254970e-04,\n",
       "        6.50640518e-04, 3.98092142e-04, 5.58433709e-04, 4.99502972e-04,\n",
       "        6.44142761e-04, 4.50511301e-04, 5.46815244e-04, 1.58083832e-04,\n",
       "        4.92907901e-04, 1.61862373e-04, 6.55053630e-04, 4.88547734e-04,\n",
       "        3.82258337e-04, 5.79691425e-05, 6.05122728e-04, 3.99040403e-04,\n",
       "        5.89184834e-04, 5.47871717e-04, 4.23222490e-04, 4.69938612e-04,\n",
       "        5.75684903e-04, 4.01828713e-04, 7.45909877e-04, 4.78695551e-04,\n",
       "        4.39571670e-04, 3.09452916e-04, 5.53887180e-04, 4.88943472e-04,\n",
       "        4.52998565e-04, 4.69613072e-04, 7.38358496e-04, 4.00689474e-04,\n",
       "        5.05223429e-04, 2.99458022e-04, 6.41647471e-04, 4.58992739e-04,\n",
       "        5.77510564e-04, 4.86300080e-04, 5.85716175e-04, 4.02502397e-04,\n",
       "        5.84398716e-04, 3.94941167e-04, 5.42530726e-04, 3.01616706e-04,\n",
       "        6.27062358e-04, 3.94446044e-04, 5.43462693e-04, 4.57608779e-04,\n",
       "        4.57060246e-04, 3.97557541e-04, 4.02691052e-04, 4.01577090e-04,\n",
       "        6.44323259e-04, 2.36403653e-05, 5.97023069e-04, 0.00000000e+00,\n",
       "        3.06086426e-04, 4.91230011e-04, 6.40519091e-04, 5.38939796e-04,\n",
       "        6.70626872e-04, 4.04158213e-04, 3.99077837e-04, 4.51248135e-04,\n",
       "        8.31669724e-04, 4.57174722e-04, 5.77286655e-04, 4.57127634e-04,\n",
       "        5.92315080e-04, 3.07951705e-04, 6.99076397e-04, 5.05315661e-04,\n",
       "        8.17678909e-04, 4.00346699e-04, 7.14259257e-04, 3.03968037e-04,\n",
       "        4.79494456e-04, 4.89082606e-04, 5.93919856e-04, 4.61098135e-04,\n",
       "        4.85376771e-04, 4.88694469e-04, 4.22694033e-04, 3.62973272e-04,\n",
       "        6.59135833e-04, 3.55377105e-04, 6.39215353e-04, 3.00353458e-04,\n",
       "        6.52607481e-04, 3.52103348e-04, 5.21238397e-04, 2.99609943e-04,\n",
       "        6.57914583e-04, 5.32145891e-04, 4.31106637e-04, 2.99406854e-04,\n",
       "        4.66569785e-04, 3.00335884e-04, 6.02931686e-04, 5.52511174e-04,\n",
       "        6.04715775e-04, 3.00552409e-04, 5.94164374e-04, 3.94633120e-04,\n",
       "        6.28469848e-04, 3.97796091e-04, 6.41071630e-04, 4.91086485e-04,\n",
       "        5.07408801e-04, 4.62967888e-04, 3.15569520e-04, 1.52707100e-04]),\n",
       " 'mean_score_time': array([3.43775749e-04, 1.99246407e-04, 4.01759148e-04, 2.05779076e-04,\n",
       "        2.90536880e-04, 9.96589661e-05, 0.00000000e+00, 2.99215317e-04,\n",
       "        0.00000000e+00, 3.35550308e-04, 3.51977348e-04, 9.68694687e-05,\n",
       "        1.02472305e-04, 1.99341774e-04, 3.98921967e-04, 1.02543831e-04,\n",
       "        1.98864937e-04, 1.99437141e-04, 9.97543335e-05, 9.95874405e-05,\n",
       "        2.28929520e-04, 2.99191475e-04, 2.02322006e-04, 9.97304916e-05,\n",
       "        3.99208069e-04, 2.02107430e-04, 9.68933105e-05, 3.01504135e-04,\n",
       "        2.02369690e-04, 2.99215317e-04, 1.96433067e-04, 3.01957130e-04,\n",
       "        1.96623802e-04, 1.97720528e-04, 0.00000000e+00, 2.48837471e-04,\n",
       "        2.02298164e-04, 1.99413300e-04, 2.00724602e-04, 2.99239159e-04,\n",
       "        3.69095802e-04, 3.03030014e-04, 3.64542007e-04, 0.00000000e+00,\n",
       "        9.97543335e-05, 1.02090836e-04, 3.96370888e-04, 0.00000000e+00,\n",
       "        1.96838379e-04, 4.72283363e-04, 3.89432907e-04, 1.99699402e-04,\n",
       "        9.96828079e-05, 2.99501419e-04, 3.99851799e-04, 3.02195549e-04,\n",
       "        9.96351242e-05, 1.64937973e-04, 3.01456451e-04, 1.01757050e-04,\n",
       "        4.99868393e-04, 1.02591515e-04, 1.99532509e-04, 1.00135803e-04,\n",
       "        3.93295288e-04, 1.99437141e-04, 3.98921967e-04, 1.97792053e-04,\n",
       "        2.96282768e-04, 1.96671486e-04, 2.27046013e-04, 1.03950500e-04,\n",
       "        4.01687622e-04, 9.97304916e-05, 3.01790237e-04, 1.95169449e-04,\n",
       "        3.99827957e-04, 9.67979431e-05, 1.99270248e-04, 1.99127197e-04,\n",
       "        2.99263000e-04, 1.30748749e-04, 1.99151039e-04, 1.96576118e-04,\n",
       "        2.02155113e-04, 4.86660004e-04, 2.53748894e-04, 0.00000000e+00,\n",
       "        9.94920731e-05, 2.00414658e-04, 1.96933746e-04, 1.99413300e-04,\n",
       "        1.13606453e-04, 0.00000000e+00, 1.88422203e-04, 1.02782249e-04,\n",
       "        0.00000000e+00, 1.75380707e-04, 2.01439857e-04, 3.02338600e-04,\n",
       "        9.98020172e-05, 9.89198685e-05, 2.48837471e-04, 1.02567673e-04,\n",
       "        3.70502472e-04, 2.61473656e-04, 4.15444374e-04, 3.60679626e-04,\n",
       "        1.99556351e-04, 9.97066498e-05, 2.28476524e-04, 2.01797485e-04,\n",
       "        2.03371048e-04, 2.01487541e-04, 3.08537483e-04, 2.96020508e-04,\n",
       "        9.95397568e-05, 2.99096107e-04, 3.98969650e-04, 2.98857689e-04,\n",
       "        3.02600861e-04, 2.99263000e-04, 3.03053856e-04, 1.99961662e-04,\n",
       "        3.01599503e-04, 1.96814537e-04, 1.99556351e-04, 1.02400780e-04,\n",
       "        3.93819809e-04, 1.99151039e-04, 1.34181976e-04, 2.01916695e-04,\n",
       "        1.99580193e-04, 2.99119949e-04, 3.99208069e-04, 3.49950790e-04,\n",
       "        9.98020172e-05, 0.00000000e+00, 9.97304916e-05, 3.01957130e-04,\n",
       "        0.00000000e+00, 2.99119949e-04, 9.97066498e-05, 5.95211983e-04,\n",
       "        0.00000000e+00, 9.96828079e-05, 3.96180153e-04, 3.02553177e-04,\n",
       "        3.02648544e-04, 7.96151161e-04, 4.02569771e-04, 9.96112823e-05,\n",
       "        1.20973587e-04, 1.96576118e-04, 1.99484825e-04, 3.72862816e-04,\n",
       "        2.02631950e-04, 9.97066498e-05, 7.01975822e-04, 1.02138519e-04,\n",
       "        3.98302078e-04, 9.97066498e-05, 1.82867050e-04, 1.02710724e-04,\n",
       "        3.95941734e-04, 1.98197365e-04, 4.05073166e-04, 0.00000000e+00,\n",
       "        4.29201126e-04, 2.02226639e-04, 2.02512741e-04, 1.49941444e-04,\n",
       "        9.96828079e-05, 1.01304054e-04, 9.96828079e-05, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.34160233e-05, 3.97086143e-04, 1.99079514e-04,\n",
       "        5.01894951e-04, 2.99382210e-04, 1.99365616e-04, 9.73701477e-05,\n",
       "        2.52699852e-04, 2.01129913e-04, 2.05159187e-04, 1.33132935e-04,\n",
       "        9.97304916e-05, 1.99389458e-04, 9.98497009e-05, 0.00000000e+00,\n",
       "        2.99358368e-04, 1.99413300e-04, 1.22427940e-04, 9.88483429e-05,\n",
       "        3.98230553e-04, 9.96828079e-05, 3.38244438e-04, 6.16502762e-04,\n",
       "        1.99627876e-04, 2.99119949e-04, 1.50632858e-04, 2.02178955e-04,\n",
       "        2.02298164e-04, 1.33609772e-04, 1.01566315e-04, 9.97781754e-05,\n",
       "        1.96194649e-04, 3.97539139e-04, 9.91344452e-05, 1.93381310e-04,\n",
       "        1.59764290e-04, 3.14688683e-04, 2.99286842e-04, 1.96480751e-04,\n",
       "        2.21657753e-04, 1.99413300e-04, 3.96060944e-04, 1.02519989e-04,\n",
       "        9.94920731e-05, 5.49793243e-05, 1.99604034e-04, 1.96504593e-04,\n",
       "        2.01702118e-04, 9.97781754e-05, 2.97999382e-04, 4.76074219e-04,\n",
       "        1.99007988e-04, 2.99096107e-04, 1.96361542e-04, 9.71794128e-05,\n",
       "        1.99437141e-04, 9.82761383e-05, 9.98735428e-05, 1.00564957e-04,\n",
       "        1.99699402e-04, 9.98497009e-05, 1.99460983e-04, 0.00000000e+00,\n",
       "        5.02657890e-04, 6.48188591e-04, 2.02155113e-04, 9.69171524e-05,\n",
       "        2.95782089e-04, 9.70125198e-05, 1.45339966e-04, 1.99365616e-04,\n",
       "        3.04579735e-04, 3.99422646e-04, 1.77598000e-04, 2.02751160e-04,\n",
       "        3.99208069e-04, 1.99484825e-04, 1.00278854e-04, 3.04651260e-04,\n",
       "        2.02083588e-04, 9.98020172e-05, 0.00000000e+00, 4.98509407e-04,\n",
       "        3.04317474e-04, 3.59725952e-04, 3.95846367e-04, 1.46389008e-04,\n",
       "        2.99215317e-04, 1.96051598e-04, 4.45437431e-04, 6.11305237e-05,\n",
       "        9.97066498e-05, 1.96743011e-04, 4.01616096e-04, 1.36256218e-04,\n",
       "        1.98197365e-04, 9.97066498e-05, 3.40056419e-04, 1.99699402e-04,\n",
       "        1.99437141e-04, 1.96552277e-04, 3.02147865e-04, 1.99556351e-04,\n",
       "        1.93572044e-04, 9.97066498e-05, 9.97781754e-05, 2.98833847e-04,\n",
       "        0.00000000e+00, 9.96351242e-05, 1.51848793e-04, 3.98707390e-04,\n",
       "        3.89552116e-04, 1.99389458e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.99627876e-04, 1.99341774e-04, 3.03578377e-04, 0.00000000e+00,\n",
       "        9.97304916e-05, 0.00000000e+00, 1.99580193e-04, 2.99143791e-04,\n",
       "        2.99239159e-04, 4.06336784e-04, 4.75883484e-04, 9.97543335e-05,\n",
       "        1.99890137e-04, 6.07490540e-05, 9.96351242e-05, 1.99246407e-04,\n",
       "        2.54797935e-04, 2.29954720e-04, 1.96576118e-04, 2.01988220e-04,\n",
       "        1.99103355e-04, 1.36613846e-04, 3.98755074e-04, 2.01177597e-04,\n",
       "        9.93967056e-05, 9.97781754e-05, 2.95996666e-04, 1.99031830e-04,\n",
       "        1.99460983e-04, 9.97304916e-05, 2.93898582e-04, 1.96743011e-04,\n",
       "        4.14705276e-04, 1.99460983e-04, 4.01854515e-04, 9.77754593e-05,\n",
       "        0.00000000e+00, 9.97781754e-05, 2.01153755e-04, 2.86841393e-04,\n",
       "        2.02345848e-04, 0.00000000e+00, 3.65400314e-04, 3.98874283e-04,\n",
       "        4.01854515e-04, 1.99460983e-04, 5.03087044e-04, 1.99437141e-04,\n",
       "        2.96568871e-04, 0.00000000e+00, 2.01821327e-04, 2.98523903e-04,\n",
       "        2.96258926e-04, 2.02059746e-04, 2.50196457e-04, 1.32513046e-04,\n",
       "        0.00000000e+00, 2.98166275e-04, 1.99747086e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.04181671e-04, 3.02290916e-04, 1.99580193e-04,\n",
       "        2.99096107e-04, 3.02433968e-04, 3.02004814e-04, 1.99699402e-04,\n",
       "        4.47964668e-04, 1.99460983e-04, 2.98047066e-04, 1.15060806e-04,\n",
       "        0.00000000e+00, 1.02543831e-04, 1.99556351e-04, 9.96589661e-05,\n",
       "        3.98921967e-04, 9.93728638e-05, 2.99406052e-04, 9.98020172e-05,\n",
       "        9.97066498e-05, 9.98973846e-05, 1.99508667e-04, 0.00000000e+00,\n",
       "        1.98149681e-04, 4.03642654e-04, 1.42145157e-04, 9.96351242e-05,\n",
       "        1.98936462e-04, 0.00000000e+00, 4.34350967e-04, 0.00000000e+00,\n",
       "        2.02393532e-04, 0.00000000e+00, 1.00183487e-04, 4.09126282e-04,\n",
       "        1.99437141e-04, 0.00000000e+00, 0.00000000e+00, 9.95159149e-05,\n",
       "        9.71555710e-05, 6.59465790e-05, 1.15823746e-04, 0.00000000e+00,\n",
       "        9.69171524e-05, 8.96906853e-04, 3.67736816e-04, 3.01051140e-04,\n",
       "        9.98973846e-05, 1.02567673e-04, 4.03594971e-04, 1.02496147e-04,\n",
       "        9.96112823e-05, 9.96828079e-05, 3.71932983e-05, 1.99413300e-04,\n",
       "        9.97781754e-05, 1.01017952e-04, 3.98683548e-04, 1.99747086e-04,\n",
       "        1.02424622e-04, 1.39117241e-04, 3.97062302e-04, 3.32450867e-04,\n",
       "        1.02376938e-04, 1.99580193e-04, 1.99437141e-04, 1.97124481e-04,\n",
       "        9.96589661e-05, 9.60588455e-05, 9.93013382e-05, 1.96409225e-04,\n",
       "        1.96623802e-04, 1.99508667e-04, 2.00057030e-04, 2.96306610e-04,\n",
       "        9.97066498e-05, 1.89661980e-04, 9.97304916e-05, 9.96589661e-05,\n",
       "        2.99954414e-04, 2.34484673e-04, 1.96647644e-04, 1.81341171e-04,\n",
       "        1.13224983e-04, 1.99270248e-04, 2.96258926e-04, 2.02178955e-04,\n",
       "        2.93612480e-04, 0.00000000e+00, 2.10881233e-04, 5.98144531e-04,\n",
       "        2.95972824e-04, 0.00000000e+00, 9.97543335e-05, 1.96957588e-04,\n",
       "        4.32491302e-05, 9.68933105e-05, 3.33595276e-04, 2.46810913e-04,\n",
       "        2.00700760e-04, 9.97543335e-05, 1.99389458e-04, 2.99263000e-04,\n",
       "        3.97753716e-04, 3.30042839e-04, 3.16190720e-04, 1.99508667e-04,\n",
       "        2.02131271e-04, 9.97304916e-05, 2.90417671e-04, 9.57250595e-05,\n",
       "        4.97722626e-04, 9.96351242e-05, 0.00000000e+00, 3.55911255e-04,\n",
       "        5.56349754e-04, 2.99048424e-04, 8.61644745e-05, 1.98340416e-04,\n",
       "        4.01639938e-04, 9.97781754e-05, 2.01964378e-04, 9.97304916e-05,\n",
       "        4.01902199e-04, 7.38382339e-05, 2.96354294e-04, 9.94205475e-05,\n",
       "        1.96385384e-04, 0.00000000e+00, 2.96521187e-04, 1.02591515e-04,\n",
       "        2.49958038e-04, 3.99351120e-04, 1.52230263e-04, 2.99048424e-04,\n",
       "        1.99484825e-04, 1.02043152e-04, 2.77280807e-05, 1.17278099e-04,\n",
       "        1.53851509e-04, 2.46953964e-04, 4.33468819e-04, 1.96695328e-04,\n",
       "        3.98755074e-04, 3.52764130e-04, 3.03268433e-04, 4.74333763e-04]),\n",
       " 'std_score_time': array([4.47215978e-04, 3.98492902e-04, 4.92110068e-04, 3.96969575e-04,\n",
       "        4.43962080e-04, 2.98976898e-04, 0.00000000e+00, 4.57235531e-04,\n",
       "        0.00000000e+00, 4.46343882e-04, 4.47442010e-04, 2.90608406e-04,\n",
       "        3.07416916e-04, 3.98685776e-04, 4.88577923e-04, 3.07631493e-04,\n",
       "        3.97732879e-04, 3.98874286e-04, 2.99263000e-04, 2.98762321e-04,\n",
       "        3.65821761e-04, 4.57023435e-04, 4.04696281e-04, 2.99191475e-04,\n",
       "        4.89081859e-04, 4.04261351e-04, 2.90679932e-04, 4.60600176e-04,\n",
       "        4.04793363e-04, 4.57061064e-04, 3.92907546e-04, 4.61296950e-04,\n",
       "        3.93300509e-04, 3.95462362e-04, 0.00000000e+00, 4.01725968e-04,\n",
       "        4.04651206e-04, 3.98827754e-04, 4.01454588e-04, 4.57280426e-04,\n",
       "        4.59940380e-04, 4.62925316e-04, 4.58416697e-04, 0.00000000e+00,\n",
       "        2.99263000e-04, 3.06272507e-04, 4.85501117e-04, 0.00000000e+00,\n",
       "        3.93722036e-04, 4.79773107e-04, 4.42175113e-04, 3.99398932e-04,\n",
       "        2.99048424e-04, 4.57499184e-04, 4.89727773e-04, 4.61674949e-04,\n",
       "        2.98905373e-04, 3.38594689e-04, 4.60525308e-04, 3.05271149e-04,\n",
       "        5.00111317e-04, 3.07774544e-04, 3.99065107e-04, 3.00407410e-04,\n",
       "        4.81761777e-04, 3.98874286e-04, 4.88766738e-04, 3.95599755e-04,\n",
       "        4.52649412e-04, 3.93395864e-04, 3.88366771e-04, 3.09237794e-04,\n",
       "        4.92062953e-04, 2.99191475e-04, 4.61041144e-04, 3.90339029e-04,\n",
       "        4.89807329e-04, 2.90393829e-04, 3.98709918e-04, 3.98254395e-04,\n",
       "        4.57318355e-04, 3.03294901e-04, 3.98302110e-04, 3.93205155e-04,\n",
       "        4.04353530e-04, 4.46402010e-04, 4.05462574e-04, 0.00000000e+00,\n",
       "        2.98476219e-04, 4.01103221e-04, 3.93919445e-04, 3.98826727e-04,\n",
       "        3.40819359e-04, 0.00000000e+00, 3.76865617e-04, 3.08346748e-04,\n",
       "        0.00000000e+00, 3.54631062e-04, 4.02929662e-04, 4.61878000e-04,\n",
       "        2.99406052e-04, 2.96759605e-04, 4.05366404e-04, 3.07703018e-04,\n",
       "        4.60699287e-04, 4.11378806e-04, 4.72676053e-04, 4.55546745e-04,\n",
       "        3.99113057e-04, 2.99119949e-04, 3.89082394e-04, 4.03658190e-04,\n",
       "        4.06745240e-04, 4.03003015e-04, 4.71897473e-04, 4.52627424e-04,\n",
       "        2.98619270e-04, 4.56876880e-04, 4.88636085e-04, 4.56536870e-04,\n",
       "        4.62317869e-04, 4.57140190e-04, 4.62984381e-04, 3.99925915e-04,\n",
       "        4.60760939e-04, 3.93676816e-04, 3.99112716e-04, 3.07202339e-04,\n",
       "        4.82433533e-04, 3.98303109e-04, 3.11429806e-04, 4.03903344e-04,\n",
       "        3.99353617e-04, 4.56913357e-04, 4.88934080e-04, 4.48631358e-04,\n",
       "        2.99406052e-04, 0.00000000e+00, 2.99191475e-04, 4.61476873e-04,\n",
       "        0.00000000e+00, 4.56913282e-04, 2.99119949e-04, 4.86052459e-04,\n",
       "        0.00000000e+00, 2.99048424e-04, 4.85275070e-04, 4.62184953e-04,\n",
       "        4.62382482e-04, 3.98394907e-04, 4.93124937e-04, 2.98833847e-04,\n",
       "        2.94273364e-04, 3.93205155e-04, 3.98969739e-04, 4.63778466e-04,\n",
       "        4.05304045e-04, 2.99119949e-04, 4.59607950e-04, 3.06415558e-04,\n",
       "        4.88105771e-04, 2.99119949e-04, 3.67633100e-04, 3.08132172e-04,\n",
       "        4.84990556e-04, 3.96404800e-04, 4.96182757e-04, 0.00000000e+00,\n",
       "        5.50208249e-04, 4.04507299e-04, 4.05084774e-04, 3.15436168e-04,\n",
       "        2.99048424e-04, 3.03912163e-04, 2.99048424e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.30248070e-04, 4.86554356e-04, 3.98389227e-04,\n",
       "        5.01954174e-04, 4.57313958e-04, 3.98731232e-04, 2.92110443e-04,\n",
       "        4.04377372e-04, 3.98059481e-04, 4.10321287e-04, 3.03862776e-04,\n",
       "        2.99191475e-04, 3.98779090e-04, 2.99549103e-04, 0.00000000e+00,\n",
       "        4.57436179e-04, 3.98826599e-04, 2.99374098e-04, 2.96545029e-04,\n",
       "        4.87895950e-04, 2.99048424e-04, 4.42622792e-04, 5.05744062e-04,\n",
       "        3.99255842e-04, 4.57095528e-04, 3.16342011e-04, 4.04413701e-04,\n",
       "        4.04641175e-04, 3.05155053e-04, 3.04698944e-04, 2.99334526e-04,\n",
       "        3.92447706e-04, 4.87063485e-04, 2.97403336e-04, 3.86762917e-04,\n",
       "        3.32617459e-04, 4.49398334e-04, 4.57168220e-04, 3.93011003e-04,\n",
       "        3.93533320e-04, 3.98826599e-04, 4.85159135e-04, 3.07559967e-04,\n",
       "        2.98476219e-04, 1.64937973e-04, 3.99208297e-04, 3.93059530e-04,\n",
       "        4.03411690e-04, 2.99334526e-04, 4.55207773e-04, 4.79660364e-04,\n",
       "        3.98016579e-04, 4.56876905e-04, 3.92775187e-04, 2.91538239e-04,\n",
       "        3.98874372e-04, 2.94828415e-04, 2.99620628e-04, 3.01694870e-04,\n",
       "        3.99398932e-04, 2.99549103e-04, 3.98922024e-04, 0.00000000e+00,\n",
       "        5.02972188e-04, 5.52513687e-04, 4.04370540e-04, 2.90751457e-04,\n",
       "        4.51876026e-04, 2.91037560e-04, 3.14620109e-04, 3.98926335e-04,\n",
       "        4.65307307e-04, 4.89350019e-04, 3.58632274e-04, 4.05572978e-04,\n",
       "        4.88928733e-04, 3.98969654e-04, 3.00836563e-04, 4.65413107e-04,\n",
       "        4.04224779e-04, 2.99406052e-04, 0.00000000e+00, 4.98533117e-04,\n",
       "        4.64916892e-04, 4.53419509e-04, 4.84879585e-04, 3.15714887e-04,\n",
       "        4.57058963e-04, 3.92137294e-04, 4.77627636e-04, 1.83391571e-04,\n",
       "        2.99119949e-04, 3.93525083e-04, 4.91938076e-04, 3.06540234e-04,\n",
       "        3.96403337e-04, 2.99119949e-04, 4.43672045e-04, 3.99399316e-04,\n",
       "        3.99067654e-04, 3.93149898e-04, 4.61591744e-04, 3.99112758e-04,\n",
       "        3.87144915e-04, 2.99119949e-04, 2.99334526e-04, 4.56620134e-04,\n",
       "        0.00000000e+00, 2.98905373e-04, 3.21431901e-04, 4.88315021e-04,\n",
       "        4.77153547e-04, 3.98917191e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.99256183e-04, 3.98683552e-04, 4.62333795e-04, 0.00000000e+00,\n",
       "        2.99191475e-04, 0.00000000e+00, 3.99160474e-04, 4.56949693e-04,\n",
       "        4.57095742e-04, 4.97977565e-04, 4.81808426e-04, 2.99263000e-04,\n",
       "        3.99781183e-04, 1.57638672e-04, 2.98905373e-04, 3.98728284e-04,\n",
       "        4.03887323e-04, 3.94051939e-04, 3.93205155e-04, 4.04017469e-04,\n",
       "        3.98207000e-04, 3.07200632e-04, 4.88373305e-04, 4.02467084e-04,\n",
       "        2.98190117e-04, 2.99334526e-04, 4.52206442e-04, 3.98064816e-04,\n",
       "        3.98922024e-04, 2.99191475e-04, 4.49006362e-04, 3.93534617e-04,\n",
       "        5.09766524e-04, 3.98922024e-04, 4.92236278e-04, 2.93326378e-04,\n",
       "        0.00000000e+00, 2.99334526e-04, 4.02369989e-04, 4.39502563e-04,\n",
       "        4.04743105e-04, 0.00000000e+00, 4.59340729e-04, 4.88519331e-04,\n",
       "        4.92234812e-04, 3.98921981e-04, 5.03145788e-04, 3.98874286e-04,\n",
       "        4.53065672e-04, 0.00000000e+00, 4.03680003e-04, 4.56003820e-04,\n",
       "        4.52743764e-04, 4.04152577e-04, 3.88348858e-04, 3.03080822e-04,\n",
       "        0.00000000e+00, 4.55581475e-04, 3.99494299e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.08369481e-04, 4.61805309e-04, 3.99161186e-04,\n",
       "        4.56877004e-04, 4.62032068e-04, 4.61383173e-04, 3.99399501e-04,\n",
       "        4.67324225e-04, 3.98921981e-04, 4.55393709e-04, 2.93795626e-04,\n",
       "        0.00000000e+00, 3.07631493e-04, 3.99112716e-04, 2.98976898e-04,\n",
       "        4.88577714e-04, 2.98118591e-04, 4.57350758e-04, 2.99406052e-04,\n",
       "        2.99119949e-04, 2.99692154e-04, 3.99017462e-04, 0.00000000e+00,\n",
       "        3.96360895e-04, 4.94410856e-04, 3.14892891e-04, 2.98905373e-04,\n",
       "        3.98096094e-04, 0.00000000e+00, 5.18195625e-04, 0.00000000e+00,\n",
       "        4.04824307e-04, 0.00000000e+00, 3.00550461e-04, 4.87345527e-04,\n",
       "        3.98874315e-04, 0.00000000e+00, 0.00000000e+00, 2.98547745e-04,\n",
       "        2.91466713e-04, 1.97839737e-04, 2.97584913e-04, 0.00000000e+00,\n",
       "        2.90751457e-04, 2.99315976e-04, 4.60009087e-04, 4.59866933e-04,\n",
       "        2.99692154e-04, 3.07703018e-04, 4.94468830e-04, 3.07488441e-04,\n",
       "        2.98833847e-04, 2.99048424e-04, 1.11579895e-04, 3.98830718e-04,\n",
       "        2.99334526e-04, 3.03053856e-04, 4.88463322e-04, 3.99720005e-04,\n",
       "        3.07273865e-04, 3.09506571e-04, 4.86374406e-04, 5.16917940e-04,\n",
       "        3.07130814e-04, 3.99160417e-04, 3.98874315e-04, 3.94304382e-04,\n",
       "        2.98976898e-04, 2.88176537e-04, 2.97904015e-04, 3.92862222e-04,\n",
       "        3.93295393e-04, 3.99017847e-04, 4.00114859e-04, 4.52806757e-04,\n",
       "        2.99119949e-04, 3.79962436e-04, 2.99191475e-04, 2.98976898e-04,\n",
       "        4.58189454e-04, 3.79191828e-04, 3.93347316e-04, 3.63676283e-04,\n",
       "        3.39674950e-04, 3.98540511e-04, 4.52604818e-04, 4.04405197e-04,\n",
       "        4.48561151e-04, 0.00000000e+00, 4.22496656e-04, 4.88555887e-04,\n",
       "        4.52184116e-04, 0.00000000e+00, 2.99263000e-04, 3.93954948e-04,\n",
       "        1.29747391e-04, 2.90679932e-04, 4.46301721e-04, 4.00503443e-04,\n",
       "        4.01411843e-04, 2.99263000e-04, 3.98778947e-04, 4.57131821e-04,\n",
       "        4.87249034e-04, 4.37115118e-04, 4.48531200e-04, 3.99017690e-04,\n",
       "        4.04314860e-04, 2.99191475e-04, 4.44071542e-04, 2.87175179e-04,\n",
       "        4.97725748e-04, 2.98905373e-04, 0.00000000e+00, 4.54474716e-04,\n",
       "        4.67767655e-04, 4.56804946e-04, 2.58493423e-04, 3.96979928e-04,\n",
       "        4.91968234e-04, 2.99334526e-04, 4.03966079e-04, 2.99191475e-04,\n",
       "        4.92293955e-04, 2.21514702e-04, 4.52760799e-04, 2.98261642e-04,\n",
       "        3.92815343e-04, 0.00000000e+00, 4.53021413e-04, 3.07774544e-04,\n",
       "        3.98899680e-04, 4.89104451e-04, 3.20784224e-04, 4.56804013e-04,\n",
       "        3.98969682e-04, 3.06129456e-04, 8.31842422e-05, 3.06370461e-04,\n",
       "        3.24163952e-04, 5.05128816e-04, 4.70423981e-04, 3.93444419e-04,\n",
       "        4.88519311e-04, 4.50041050e-04, 4.63303827e-04, 4.80912393e-04]),\n",
       " 'param_criterion': masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 15, 15, 15, 15, 15, 15,\n",
       "                    15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "                    15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "                    15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "                    15, 15, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "                    20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "                    20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "                    20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 30, 30, 30, 30,\n",
       "                    30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
       "                    30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
       "                    30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
       "                    30, 30, 30, 30, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "                    15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "                    15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "                    15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "                    20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "                    20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "                    20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "                    20, 20, 20, 20, 20, 20, 20, 20, 30, 30, 30, 30, 30, 30,\n",
       "                    30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
       "                    30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
       "                    30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
       "                    30, 30, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 8, 8, 8, 8,\n",
       "                    8, 8, 8, 8, 8, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "                    2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "                    8, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "                    2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 8, 8,\n",
       "                    8, 8, 8, 8, 8, 8, 8, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8,\n",
       "                    8, 8, 8, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "                    2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_split': masked_array(data=[2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8,\n",
       "                    10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5,\n",
       "                    5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2,\n",
       "                    3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10,\n",
       "                    10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5,\n",
       "                    8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3,\n",
       "                    3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10,\n",
       "                    2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8,\n",
       "                    10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5,\n",
       "                    5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2,\n",
       "                    3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10,\n",
       "                    10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5,\n",
       "                    8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3,\n",
       "                    3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10,\n",
       "                    2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8,\n",
       "                    10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5,\n",
       "                    5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2,\n",
       "                    3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10,\n",
       "                    10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5,\n",
       "                    8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3,\n",
       "                    3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10,\n",
       "                    2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8,\n",
       "                    10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5,\n",
       "                    5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2,\n",
       "                    3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10,\n",
       "                    10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5,\n",
       "                    8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3,\n",
       "                    3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8, 10, 10,\n",
       "                    2, 2, 3, 3, 5, 5, 8, 8, 10, 10, 2, 2, 3, 3, 5, 5, 8, 8,\n",
       "                    10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_splitter': masked_array(data=['best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random', 'best', 'random', 'best', 'random',\n",
       "                    'best', 'random'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 15,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 3,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 2,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 3,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 5,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 8,\n",
       "   'splitter': 'random'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'best'},\n",
       "  {'criterion': 'entropy',\n",
       "   'max_depth': None,\n",
       "   'min_samples_leaf': 8,\n",
       "   'min_samples_split': 10,\n",
       "   'splitter': 'random'}],\n",
       " 'split0_test_score': array([0.9122807 , 0.94736842, 0.94736842, 0.89473684, 0.89473684,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.9122807 , 0.96491228,\n",
       "        0.94736842, 0.94736842, 0.96491228, 0.92982456, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.96491228, 0.92982456, 0.92982456, 0.9122807 ,\n",
       "        0.94736842, 0.89473684, 0.98245614, 0.9122807 , 0.92982456,\n",
       "        0.94736842, 0.96491228, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.94736842, 0.94736842, 0.98245614,\n",
       "        0.94736842, 0.96491228, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.98245614, 0.94736842, 0.96491228, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.9122807 , 0.94736842, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.9122807 , 0.92982456, 0.9122807 , 0.94736842,\n",
       "        0.92982456, 0.92982456, 0.94736842, 0.87719298, 0.92982456,\n",
       "        0.96491228, 0.92982456, 1.        , 0.9122807 , 0.96491228,\n",
       "        0.92982456, 0.89473684, 0.9122807 , 0.92982456, 0.89473684,\n",
       "        0.94736842, 0.9122807 , 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.94736842, 0.89473684, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.96491228, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.98245614, 0.94736842, 0.87719298, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.94736842, 0.92982456,\n",
       "        0.92982456, 0.96491228, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.9122807 , 0.9122807 , 0.96491228, 0.9122807 , 0.98245614,\n",
       "        0.92982456, 0.96491228, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.94736842, 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.89473684, 0.96491228, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.89473684, 0.92982456, 0.92982456, 0.92982456, 0.96491228,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.96491228, 0.94736842, 0.87719298,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.87719298, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.92982456, 0.89473684, 0.92982456, 0.89473684, 0.92982456,\n",
       "        0.9122807 , 0.9122807 , 0.98245614, 0.9122807 , 0.94736842,\n",
       "        0.96491228, 0.98245614, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.96491228, 0.89473684, 0.92982456, 0.9122807 , 0.89473684,\n",
       "        0.9122807 , 0.9122807 , 0.92982456, 0.92982456, 0.94736842,\n",
       "        1.        , 0.89473684, 0.98245614, 0.92982456, 0.9122807 ,\n",
       "        0.94736842, 0.96491228, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.98245614, 0.94736842, 0.96491228, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.9122807 , 0.94736842, 0.89473684, 0.94736842, 0.92982456,\n",
       "        0.9122807 , 0.96491228, 0.92982456, 0.98245614, 0.9122807 ,\n",
       "        0.94736842, 0.9122807 , 0.94736842, 0.92982456, 0.85964912,\n",
       "        0.9122807 , 0.96491228, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.9122807 , 0.9122807 , 0.96491228, 0.9122807 , 0.96491228,\n",
       "        0.92982456, 0.92982456, 0.9122807 , 0.9122807 , 0.94736842,\n",
       "        0.98245614, 0.9122807 , 0.92982456, 0.9122807 , 0.96491228,\n",
       "        0.94736842, 0.96491228, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.96491228, 0.94736842, 0.98245614,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.85964912, 0.94736842, 0.94736842, 0.94736842, 0.96491228,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.94736842, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.92982456, 0.98245614,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.89473684, 0.92982456, 0.94736842,\n",
       "        0.94736842, 0.96491228, 0.9122807 , 0.94736842, 0.92982456,\n",
       "        0.87719298, 0.92982456, 0.94736842, 0.92982456, 0.96491228,\n",
       "        0.9122807 , 0.98245614, 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.96491228, 0.9122807 , 0.96491228, 0.9122807 , 0.96491228,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.98245614, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.92982456, 1.        , 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.96491228, 0.92982456, 0.9122807 ,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.98245614, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.94736842, 0.92982456, 0.96491228,\n",
       "        0.9122807 , 0.92982456, 0.9122807 , 0.92982456, 0.92982456,\n",
       "        0.94736842, 0.9122807 , 0.92982456, 0.92982456, 0.96491228,\n",
       "        0.9122807 , 0.9122807 , 0.9122807 , 0.9122807 , 0.9122807 ,\n",
       "        0.94736842, 0.9122807 , 0.98245614, 0.9122807 , 0.94736842,\n",
       "        0.92982456, 0.96491228, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.89473684, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.89473684, 0.92982456, 0.98245614,\n",
       "        0.92982456, 0.98245614, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.89473684, 0.92982456, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.9122807 , 0.94736842, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.9122807 , 0.98245614, 0.9122807 , 0.96491228, 0.9122807 ,\n",
       "        0.96491228, 0.9122807 , 0.9122807 , 0.9122807 , 0.94736842,\n",
       "        0.92982456, 0.96491228, 0.92982456, 0.89473684, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.9122807 , 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.92982456, 0.94736842, 0.94736842, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.92982456, 0.94736842, 0.89473684,\n",
       "        0.9122807 , 0.96491228, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.9122807 , 0.9122807 , 0.94736842, 0.9122807 , 0.92982456,\n",
       "        0.92982456, 0.98245614, 0.92982456, 0.85964912, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.92982456, 0.94736842,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.9122807 , 0.98245614, 0.92982456, 0.9122807 , 0.9122807 ,\n",
       "        0.92982456, 0.94736842, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.9122807 , 0.92982456, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.94736842, 0.9122807 , 0.9122807 , 0.9122807 , 0.96491228]),\n",
       " 'split1_test_score': array([0.84210526, 0.94736842, 0.85964912, 0.85964912, 0.87719298,\n",
       "        0.85964912, 0.8245614 , 0.8245614 , 0.8245614 , 0.9122807 ,\n",
       "        0.85964912, 0.92982456, 0.84210526, 0.98245614, 0.84210526,\n",
       "        0.78947368, 0.8245614 , 0.89473684, 0.8245614 , 0.8245614 ,\n",
       "        0.84210526, 0.87719298, 0.84210526, 0.89473684, 0.84210526,\n",
       "        0.85964912, 0.8245614 , 0.89473684, 0.8245614 , 0.89473684,\n",
       "        0.8245614 , 0.89473684, 0.8245614 , 0.87719298, 0.8245614 ,\n",
       "        0.89473684, 0.8245614 , 0.89473684, 0.8245614 , 0.89473684,\n",
       "        0.89473684, 0.9122807 , 0.89473684, 0.89473684, 0.89473684,\n",
       "        0.87719298, 0.89473684, 0.87719298, 0.89473684, 0.89473684,\n",
       "        0.87719298, 0.87719298, 0.89473684, 0.92982456, 0.87719298,\n",
       "        0.89473684, 0.8245614 , 0.9122807 , 0.84210526, 0.92982456,\n",
       "        0.87719298, 0.89473684, 0.85964912, 0.87719298, 0.85964912,\n",
       "        0.89473684, 0.8245614 , 0.89473684, 0.8245614 , 0.84210526,\n",
       "        0.84210526, 0.89473684, 0.84210526, 0.89473684, 0.84210526,\n",
       "        0.84210526, 0.8245614 , 0.84210526, 0.8245614 , 0.87719298,\n",
       "        0.8245614 , 0.85964912, 0.8245614 , 0.85964912, 0.8245614 ,\n",
       "        0.8245614 , 0.8245614 , 0.87719298, 0.8245614 , 0.89473684,\n",
       "        0.89473684, 0.87719298, 0.89473684, 0.84210526, 0.89473684,\n",
       "        0.89473684, 0.89473684, 0.9122807 , 0.89473684, 0.87719298,\n",
       "        0.85964912, 0.9122807 , 0.84210526, 0.92982456, 0.87719298,\n",
       "        0.8245614 , 0.84210526, 0.89473684, 0.8245614 , 0.85964912,\n",
       "        0.85964912, 0.89473684, 0.89473684, 0.94736842, 0.89473684,\n",
       "        0.85964912, 0.8245614 , 0.85964912, 0.8245614 , 0.92982456,\n",
       "        0.84210526, 0.89473684, 0.84210526, 0.85964912, 0.84210526,\n",
       "        0.89473684, 0.8245614 , 0.85964912, 0.8245614 , 0.87719298,\n",
       "        0.8245614 , 0.87719298, 0.8245614 , 0.89473684, 0.8245614 ,\n",
       "        0.85964912, 0.8245614 , 0.9122807 , 0.8245614 , 0.89473684,\n",
       "        0.89473684, 0.89473684, 0.89473684, 0.85964912, 0.89473684,\n",
       "        0.89473684, 0.89473684, 0.85964912, 0.89473684, 0.84210526,\n",
       "        0.87719298, 0.87719298, 0.85964912, 0.84210526, 0.85964912,\n",
       "        0.94736842, 0.8245614 , 0.87719298, 0.84210526, 0.84210526,\n",
       "        0.85964912, 0.87719298, 0.84210526, 0.9122807 , 0.85964912,\n",
       "        0.87719298, 0.8245614 , 0.92982456, 0.8245614 , 0.85964912,\n",
       "        0.84210526, 0.8245614 , 0.84210526, 0.92982456, 0.84210526,\n",
       "        0.85964912, 0.8245614 , 0.9122807 , 0.8245614 , 0.89473684,\n",
       "        0.8245614 , 0.9122807 , 0.8245614 , 0.84210526, 0.8245614 ,\n",
       "        0.87719298, 0.8245614 , 0.89473684, 0.8245614 , 0.96491228,\n",
       "        0.89473684, 0.87719298, 0.89473684, 0.9122807 , 0.89473684,\n",
       "        0.9122807 , 0.89473684, 0.8245614 , 0.89473684, 0.89473684,\n",
       "        0.85964912, 0.89473684, 0.87719298, 0.89473684, 0.85964912,\n",
       "        0.87719298, 0.84210526, 0.9122807 , 0.84210526, 0.87719298,\n",
       "        0.89473684, 0.89473684, 0.87719298, 0.9122807 , 0.85964912,\n",
       "        0.87719298, 0.8245614 , 0.8245614 , 0.8245614 , 0.89473684,\n",
       "        0.84210526, 0.84210526, 0.84210526, 0.84210526, 0.84210526,\n",
       "        0.87719298, 0.8245614 , 0.87719298, 0.8245614 , 0.9122807 ,\n",
       "        0.8245614 , 0.85964912, 0.8245614 , 0.85964912, 0.8245614 ,\n",
       "        0.89473684, 0.8245614 , 0.8245614 , 0.8245614 , 0.84210526,\n",
       "        0.89473684, 0.85964912, 0.89473684, 0.84210526, 0.89473684,\n",
       "        0.92982456, 0.89473684, 0.9122807 , 0.89473684, 0.87719298,\n",
       "        0.9122807 , 0.87719298, 0.89473684, 0.92982456, 0.85964912,\n",
       "        0.92982456, 0.87719298, 0.9122807 , 0.85964912, 0.85964912,\n",
       "        0.84210526, 0.9122807 , 0.85964912, 0.92982456, 0.85964912,\n",
       "        0.85964912, 0.85964912, 0.87719298, 0.85964912, 0.9122807 ,\n",
       "        0.89473684, 0.9122807 , 0.84210526, 0.92982456, 0.87719298,\n",
       "        0.89473684, 0.84210526, 0.92982456, 0.84210526, 0.87719298,\n",
       "        0.84210526, 0.85964912, 0.9122807 , 0.89473684, 0.84210526,\n",
       "        0.8245614 , 0.9122807 , 0.92982456, 0.9122807 , 0.80701754,\n",
       "        0.89473684, 0.87719298, 0.8245614 , 0.9122807 , 0.89473684,\n",
       "        0.8245614 , 0.8245614 , 0.85964912, 0.8245614 , 0.87719298,\n",
       "        0.85964912, 0.89473684, 0.89473684, 0.89473684, 0.87719298,\n",
       "        0.94736842, 0.85964912, 0.92982456, 0.9122807 , 0.9122807 ,\n",
       "        0.87719298, 0.85964912, 0.8245614 , 0.89473684, 0.8245614 ,\n",
       "        0.85964912, 0.89473684, 0.85964912, 0.85964912, 0.85964912,\n",
       "        0.89473684, 0.89473684, 0.87719298, 0.84210526, 0.89473684,\n",
       "        0.85964912, 0.89473684, 0.9122807 , 0.89473684, 0.85964912,\n",
       "        0.84210526, 0.87719298, 0.9122807 , 0.84210526, 0.9122807 ,\n",
       "        0.87719298, 0.9122807 , 0.84210526, 0.9122807 , 0.9122807 ,\n",
       "        0.89473684, 0.87719298, 0.8245614 , 0.89473684, 0.89473684,\n",
       "        0.9122807 , 0.8245614 , 0.89473684, 0.8245614 , 0.92982456,\n",
       "        0.89473684, 0.9122807 , 0.85964912, 0.89473684, 0.80701754,\n",
       "        0.89473684, 0.87719298, 0.87719298, 0.85964912, 0.87719298,\n",
       "        0.8245614 , 0.87719298, 0.9122807 , 0.89473684, 0.89473684,\n",
       "        0.87719298, 0.85964912, 0.84210526, 0.89473684, 0.87719298,\n",
       "        0.87719298, 0.80701754, 0.87719298, 0.94736842, 0.84210526,\n",
       "        0.85964912, 0.84210526, 0.87719298, 0.89473684, 0.8245614 ,\n",
       "        0.84210526, 0.85964912, 0.9122807 , 0.89473684, 0.84210526,\n",
       "        0.92982456, 0.9122807 , 0.85964912, 0.84210526, 0.89473684,\n",
       "        0.8245614 , 0.84210526, 0.8245614 , 0.9122807 , 0.89473684,\n",
       "        0.85964912, 0.89473684, 0.92982456, 0.89473684, 0.92982456,\n",
       "        0.85964912, 0.89473684, 0.87719298, 0.92982456, 0.87719298,\n",
       "        0.84210526, 0.87719298, 0.89473684, 0.89473684, 0.84210526,\n",
       "        0.85964912, 0.89473684, 0.85964912, 0.89473684, 0.89473684,\n",
       "        0.84210526, 0.85964912, 0.92982456, 0.9122807 , 0.85964912,\n",
       "        0.87719298, 0.9122807 , 0.87719298, 0.89473684, 0.87719298,\n",
       "        0.89473684, 0.87719298, 0.80701754, 0.87719298, 0.87719298,\n",
       "        0.9122807 , 0.87719298, 0.84210526, 0.89473684, 0.9122807 ,\n",
       "        0.87719298, 0.84210526, 0.87719298, 0.9122807 , 0.85964912,\n",
       "        0.8245614 , 0.84210526, 0.89473684, 0.87719298, 0.8245614 ,\n",
       "        0.85964912, 0.89473684, 0.96491228, 0.8245614 , 0.89473684,\n",
       "        0.85964912, 0.84210526, 0.84210526, 0.9122807 , 0.87719298,\n",
       "        0.84210526, 0.85964912, 0.89473684, 0.85964912, 0.89473684,\n",
       "        0.85964912, 0.89473684, 0.85964912, 0.85964912, 0.85964912,\n",
       "        0.84210526, 0.85964912, 0.85964912, 0.89473684, 0.92982456,\n",
       "        0.89473684, 0.87719298, 0.87719298, 0.92982456, 0.9122807 ,\n",
       "        0.9122807 , 0.89473684, 0.87719298, 0.87719298, 0.85964912,\n",
       "        0.9122807 , 0.8245614 , 0.84210526, 0.85964912, 0.9122807 ,\n",
       "        0.87719298, 0.84210526, 0.85964912, 0.9122807 , 0.84210526,\n",
       "        0.8245614 , 0.92982456, 0.8245614 , 0.87719298, 0.8245614 ,\n",
       "        0.85964912, 0.89473684, 0.87719298, 0.89473684, 0.87719298]),\n",
       " 'split2_test_score': array([0.92982456, 0.9122807 , 0.92982456, 0.89473684, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.9122807 , 0.92982456, 0.87719298,\n",
       "        0.92982456, 0.89473684, 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.89473684, 0.92982456, 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.9122807 , 0.92982456, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.87719298, 0.9122807 , 0.9122807 , 0.9122807 , 0.9122807 ,\n",
       "        0.92982456, 0.89473684, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.98245614, 0.92982456, 0.98245614,\n",
       "        0.9122807 , 0.92982456, 0.9122807 , 0.92982456, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.9122807 , 0.92982456, 0.94736842,\n",
       "        0.9122807 , 0.9122807 , 0.92982456, 0.87719298, 0.92982456,\n",
       "        0.9122807 , 0.92982456, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.92982456, 0.9122807 ,\n",
       "        0.9122807 , 0.87719298, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.9122807 , 0.9122807 , 0.89473684, 0.9122807 , 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.89473684, 0.92982456,\n",
       "        0.98245614, 0.92982456, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.92982456, 0.9122807 ,\n",
       "        0.89473684, 0.9122807 , 0.94736842, 0.92982456, 0.98245614,\n",
       "        0.9122807 , 0.9122807 , 0.92982456, 0.89473684, 0.92982456,\n",
       "        0.9122807 , 0.92982456, 0.89473684, 0.92982456, 0.9122807 ,\n",
       "        0.94736842, 0.84210526, 0.94736842, 0.94736842, 0.9122807 ,\n",
       "        0.92982456, 0.92982456, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.9122807 , 0.9122807 , 0.9122807 , 0.9122807 , 0.9122807 ,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.92982456, 0.89473684, 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.9122807 , 0.92982456, 0.94736842, 0.92982456, 0.9122807 ,\n",
       "        0.92982456, 0.89473684, 0.92982456, 0.89473684, 0.9122807 ,\n",
       "        0.9122807 , 0.92982456, 0.96491228, 0.92982456, 0.94736842,\n",
       "        0.9122807 , 0.94736842, 0.92982456, 0.89473684, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.92982456, 0.9122807 ,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.9122807 , 0.92982456, 0.9122807 , 0.96491228, 0.9122807 ,\n",
       "        0.92982456, 0.9122807 , 0.94736842, 0.9122807 , 0.96491228,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.87719298, 0.92982456, 0.92982456,\n",
       "        0.9122807 , 0.94736842, 0.9122807 , 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.9122807 , 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.9122807 , 0.9122807 , 0.94736842, 0.92982456,\n",
       "        0.98245614, 0.92982456, 0.96491228, 0.92982456, 0.89473684,\n",
       "        0.92982456, 0.92982456, 0.94736842, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.92982456, 0.92982456, 0.89473684,\n",
       "        0.9122807 , 0.9122807 , 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.9122807 , 0.9122807 ,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.85964912, 0.92982456, 0.89473684,\n",
       "        0.9122807 , 0.92982456, 0.9122807 , 0.89473684, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.92982456, 0.9122807 , 0.94736842,\n",
       "        0.94736842, 0.92982456, 0.96491228, 0.87719298, 0.96491228,\n",
       "        0.9122807 , 0.92982456, 0.92982456, 0.92982456, 0.9122807 ,\n",
       "        0.96491228, 0.94736842, 0.96491228, 0.94736842, 0.96491228,\n",
       "        0.94736842, 0.92982456, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.96491228, 1.        , 0.96491228,\n",
       "        0.92982456, 0.92982456, 0.96491228, 0.92982456, 0.9122807 ,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.89473684, 0.92982456, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.9122807 , 0.92982456, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.85964912, 0.94736842, 0.9122807 , 0.9122807 , 0.96491228,\n",
       "        0.94736842, 0.96491228, 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.96491228, 0.9122807 , 0.96491228, 0.9122807 , 0.96491228,\n",
       "        0.94736842, 0.92982456, 0.98245614, 0.92982456, 0.94736842,\n",
       "        0.94736842, 0.96491228, 0.94736842, 0.94736842, 0.96491228,\n",
       "        0.92982456, 0.92982456, 0.89473684, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.92982456, 0.9122807 ,\n",
       "        0.9122807 , 0.9122807 , 0.94736842, 0.9122807 , 0.9122807 ,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.94736842, 0.9122807 ,\n",
       "        0.94736842, 0.89473684, 0.94736842, 0.98245614, 0.96491228,\n",
       "        0.87719298, 0.92982456, 0.9122807 , 0.92982456, 0.98245614,\n",
       "        0.96491228, 0.94736842, 0.96491228, 0.9122807 , 0.96491228,\n",
       "        0.92982456, 0.92982456, 0.89473684, 0.92982456, 0.92982456,\n",
       "        0.96491228, 0.94736842, 0.96491228, 0.89473684, 0.94736842,\n",
       "        0.96491228, 0.92982456, 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.96491228, 0.92982456, 0.9122807 ,\n",
       "        0.9122807 , 0.92982456, 0.9122807 , 0.98245614, 0.94736842,\n",
       "        0.94736842, 0.9122807 , 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.94736842, 0.89473684, 0.94736842, 0.94736842, 0.96491228,\n",
       "        0.9122807 , 0.92982456, 0.96491228, 0.92982456, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.9122807 , 0.92982456, 0.92982456, 0.92982456, 0.94736842,\n",
       "        0.96491228, 0.92982456, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.9122807 , 0.92982456, 0.94736842, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.98245614, 0.92982456, 0.89473684, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.92982456, 0.92982456, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.9122807 , 0.98245614, 0.9122807 ,\n",
       "        0.92982456, 0.94736842, 0.94736842, 0.9122807 , 0.94736842,\n",
       "        0.94736842, 0.85964912, 0.94736842, 0.87719298, 0.96491228,\n",
       "        0.92982456, 0.92982456, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.96491228, 0.94736842, 0.96491228,\n",
       "        0.87719298, 0.92982456, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.96491228, 0.94736842, 0.96491228,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.92982456, 0.89473684,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.9122807 , 0.92982456, 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.9122807 , 0.94736842, 0.9122807 , 0.89473684, 0.9122807 ,\n",
       "        0.9122807 , 0.94736842, 0.87719298, 0.94736842, 0.89473684]),\n",
       " 'split3_test_score': array([0.87719298, 1.        , 0.89473684, 0.9122807 , 0.85964912,\n",
       "        0.92982456, 0.87719298, 0.89473684, 0.87719298, 0.96491228,\n",
       "        0.89473684, 0.89473684, 0.85964912, 0.94736842, 0.87719298,\n",
       "        0.9122807 , 0.87719298, 0.92982456, 0.85964912, 0.98245614,\n",
       "        0.87719298, 0.96491228, 0.84210526, 0.92982456, 0.89473684,\n",
       "        0.94736842, 0.84210526, 0.96491228, 0.84210526, 0.94736842,\n",
       "        0.84210526, 0.94736842, 0.85964912, 0.9122807 , 0.84210526,\n",
       "        0.92982456, 0.85964912, 0.96491228, 0.84210526, 0.92982456,\n",
       "        0.85964912, 0.92982456, 0.85964912, 0.94736842, 0.85964912,\n",
       "        0.92982456, 0.85964912, 0.94736842, 0.85964912, 0.9122807 ,\n",
       "        0.89473684, 0.96491228, 0.9122807 , 0.92982456, 0.87719298,\n",
       "        0.92982456, 0.84210526, 0.92982456, 0.87719298, 0.92982456,\n",
       "        0.84210526, 0.9122807 , 0.87719298, 0.96491228, 0.84210526,\n",
       "        0.9122807 , 0.89473684, 0.92982456, 0.84210526, 0.9122807 ,\n",
       "        0.85964912, 0.89473684, 0.85964912, 0.98245614, 0.85964912,\n",
       "        0.87719298, 0.84210526, 0.92982456, 0.85964912, 0.9122807 ,\n",
       "        0.85964912, 0.94736842, 0.84210526, 0.92982456, 0.84210526,\n",
       "        0.94736842, 0.84210526, 0.89473684, 0.85964912, 0.92982456,\n",
       "        0.85964912, 0.89473684, 0.85964912, 0.9122807 , 0.85964912,\n",
       "        0.96491228, 0.85964912, 0.92982456, 0.85964912, 0.9122807 ,\n",
       "        0.87719298, 0.94736842, 0.87719298, 0.96491228, 0.87719298,\n",
       "        0.94736842, 0.9122807 , 0.92982456, 0.85964912, 0.94736842,\n",
       "        0.89473684, 0.92982456, 0.89473684, 0.9122807 , 0.85964912,\n",
       "        0.92982456, 0.87719298, 0.9122807 , 0.85964912, 0.9122807 ,\n",
       "        0.84210526, 0.89473684, 0.85964912, 0.94736842, 0.84210526,\n",
       "        0.87719298, 0.89473684, 0.96491228, 0.85964912, 0.94736842,\n",
       "        0.85964912, 0.94736842, 0.85964912, 0.89473684, 0.85964912,\n",
       "        0.94736842, 0.85964912, 0.94736842, 0.84210526, 0.94736842,\n",
       "        0.85964912, 0.89473684, 0.85964912, 0.89473684, 0.85964912,\n",
       "        0.92982456, 0.85964912, 0.96491228, 0.85964912, 0.9122807 ,\n",
       "        0.9122807 , 0.85964912, 0.87719298, 0.94736842, 0.89473684,\n",
       "        0.87719298, 0.89473684, 0.94736842, 0.84210526, 0.92982456,\n",
       "        0.89473684, 0.9122807 , 0.87719298, 0.92982456, 0.9122807 ,\n",
       "        0.89473684, 0.87719298, 0.9122807 , 0.84210526, 0.92982456,\n",
       "        0.84210526, 0.98245614, 0.85964912, 0.9122807 , 0.87719298,\n",
       "        0.85964912, 0.8245614 , 0.94736842, 0.87719298, 0.94736842,\n",
       "        0.84210526, 0.94736842, 0.84210526, 0.9122807 , 0.84210526,\n",
       "        0.9122807 , 0.85964912, 0.92982456, 0.85964912, 0.9122807 ,\n",
       "        0.85964912, 0.9122807 , 0.85964912, 0.98245614, 0.85964912,\n",
       "        0.92982456, 0.85964912, 0.94736842, 0.85964912, 0.9122807 ,\n",
       "        0.87719298, 0.92982456, 0.89473684, 0.92982456, 0.87719298,\n",
       "        0.92982456, 0.84210526, 0.92982456, 0.85964912, 0.89473684,\n",
       "        0.87719298, 0.92982456, 0.87719298, 0.9122807 , 0.87719298,\n",
       "        0.96491228, 0.89473684, 0.96491228, 0.85964912, 0.94736842,\n",
       "        0.9122807 , 0.92982456, 0.9122807 , 0.92982456, 0.85964912,\n",
       "        0.89473684, 0.9122807 , 0.94736842, 0.87719298, 0.89473684,\n",
       "        0.84210526, 0.96491228, 0.85964912, 0.94736842, 0.84210526,\n",
       "        0.9122807 , 0.85964912, 0.94736842, 0.85964912, 0.96491228,\n",
       "        0.85964912, 0.87719298, 0.85964912, 0.94736842, 0.85964912,\n",
       "        0.9122807 , 0.85964912, 0.85964912, 0.85964912, 0.92982456,\n",
       "        0.89473684, 0.94736842, 0.9122807 , 0.94736842, 0.89473684,\n",
       "        0.98245614, 0.87719298, 0.94736842, 0.87719298, 0.94736842,\n",
       "        0.89473684, 0.94736842, 0.89473684, 0.94736842, 0.89473684,\n",
       "        0.87719298, 0.87719298, 0.9122807 , 0.87719298, 0.92982456,\n",
       "        0.87719298, 0.89473684, 0.9122807 , 0.96491228, 0.9122807 ,\n",
       "        0.89473684, 0.9122807 , 0.92982456, 0.87719298, 0.9122807 ,\n",
       "        0.89473684, 0.92982456, 0.87719298, 0.9122807 , 0.89473684,\n",
       "        0.94736842, 0.87719298, 0.92982456, 0.87719298, 0.9122807 ,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.98245614, 0.94736842,\n",
       "        0.89473684, 0.92982456, 0.92982456, 0.94736842, 0.9122807 ,\n",
       "        0.89473684, 0.94736842, 0.89473684, 0.94736842, 0.85964912,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.84210526, 0.96491228,\n",
       "        0.9122807 , 0.96491228, 0.84210526, 0.92982456, 0.89473684,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.87719298, 0.92982456,\n",
       "        0.87719298, 0.9122807 , 0.9122807 , 0.92982456, 0.87719298,\n",
       "        0.96491228, 0.9122807 , 0.89473684, 0.84210526, 0.92982456,\n",
       "        0.87719298, 0.98245614, 0.89473684, 0.98245614, 0.87719298,\n",
       "        0.9122807 , 0.87719298, 0.92982456, 0.89473684, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.94736842, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.9122807 , 0.92982456, 0.94736842, 0.94736842,\n",
       "        0.89473684, 0.89473684, 0.9122807 , 0.94736842, 0.85964912,\n",
       "        0.96491228, 0.87719298, 0.92982456, 0.87719298, 0.96491228,\n",
       "        0.9122807 , 0.9122807 , 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.92982456, 0.87719298, 0.92982456, 0.87719298, 0.94736842,\n",
       "        0.9122807 , 0.94736842, 0.9122807 , 0.89473684, 0.9122807 ,\n",
       "        0.92982456, 0.9122807 , 0.96491228, 0.84210526, 0.85964912,\n",
       "        0.89473684, 0.9122807 , 0.89473684, 0.94736842, 0.89473684,\n",
       "        0.96491228, 0.89473684, 0.89473684, 0.85964912, 0.96491228,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.85964912, 0.94736842, 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.98245614, 0.87719298, 0.96491228, 0.87719298, 0.92982456,\n",
       "        0.85964912, 0.89473684, 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.96491228, 0.9122807 , 0.9122807 , 0.84210526, 0.92982456,\n",
       "        0.87719298, 0.92982456, 0.87719298, 0.92982456, 0.9122807 ,\n",
       "        0.92982456, 0.9122807 , 0.87719298, 0.87719298, 0.92982456,\n",
       "        0.87719298, 0.9122807 , 0.87719298, 0.92982456, 0.87719298,\n",
       "        0.9122807 , 0.89473684, 0.89473684, 0.89473684, 0.89473684,\n",
       "        0.92982456, 0.92982456, 0.94736842, 0.96491228, 0.9122807 ,\n",
       "        0.92982456, 0.94736842, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.9122807 , 0.94736842, 0.89473684, 0.92982456, 0.9122807 ,\n",
       "        0.9122807 , 0.87719298, 0.94736842, 0.84210526, 0.9122807 ,\n",
       "        0.87719298, 0.92982456, 0.85964912, 0.92982456, 0.85964912,\n",
       "        0.92982456, 0.9122807 , 0.89473684, 0.87719298, 0.9122807 ,\n",
       "        0.9122807 , 0.89473684, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.94736842, 0.9122807 , 0.98245614, 0.87719298, 0.92982456,\n",
       "        0.85964912, 0.92982456, 0.87719298, 0.94736842, 0.89473684,\n",
       "        0.96491228, 0.85964912, 0.94736842, 0.87719298, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.92982456, 0.89473684, 0.92982456,\n",
       "        0.98245614, 0.92982456, 0.92982456, 0.9122807 , 0.94736842]),\n",
       " 'split4_test_score': array([0.94736842, 0.89473684, 0.96491228, 0.92982456, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.96491228, 0.9122807 , 0.94736842,\n",
       "        0.98245614, 0.98245614, 0.98245614, 0.96491228, 0.96491228,\n",
       "        0.92982456, 0.92982456, 0.94736842, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.98245614, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.94736842, 0.92982456, 0.96491228,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.9122807 , 0.94736842, 0.96491228,\n",
       "        0.98245614, 0.94736842, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.92982456, 0.98245614, 0.96491228,\n",
       "        0.96491228, 0.98245614, 0.92982456, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.94736842, 0.94736842, 0.94736842, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.98245614, 0.94736842, 0.92982456,\n",
       "        0.96491228, 0.92982456, 1.        , 0.92982456, 0.98245614,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.92982456, 0.92982456, 0.94736842,\n",
       "        0.94736842, 1.        , 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.98245614, 0.94736842, 1.        ,\n",
       "        0.98245614, 0.96491228, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.94736842, 0.98245614, 0.94736842, 0.98245614, 0.98245614,\n",
       "        0.92982456, 0.96491228, 0.96491228, 0.96491228, 0.96491228,\n",
       "        0.9122807 , 0.94736842, 0.9122807 , 0.9122807 , 0.89473684,\n",
       "        0.96491228, 0.96491228, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.98245614, 0.92982456, 0.9122807 , 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.98245614, 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.94736842, 0.92982456, 0.96491228,\n",
       "        0.94736842, 0.89473684, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.96491228, 0.94736842, 0.9122807 ,\n",
       "        0.98245614, 0.98245614, 0.98245614, 0.96491228, 0.98245614,\n",
       "        0.94736842, 0.98245614, 0.94736842, 0.98245614, 0.96491228,\n",
       "        0.96491228, 0.98245614, 0.89473684, 0.96491228, 0.96491228,\n",
       "        0.94736842, 0.9122807 , 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.96491228, 0.96491228, 0.96491228,\n",
       "        1.        , 0.92982456, 0.92982456, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.98245614, 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.89473684, 0.92982456, 0.96491228, 0.92982456, 0.94736842,\n",
       "        0.94736842, 0.96491228, 0.94736842, 0.98245614, 0.94736842,\n",
       "        0.9122807 , 0.94736842, 0.94736842, 0.94736842, 0.98245614,\n",
       "        0.98245614, 1.        , 0.98245614, 0.94736842, 0.98245614,\n",
       "        0.94736842, 0.98245614, 0.98245614, 0.98245614, 0.9122807 ,\n",
       "        0.98245614, 0.94736842, 0.98245614, 0.92982456, 0.98245614,\n",
       "        1.        , 0.94736842, 0.94736842, 0.94736842, 0.98245614,\n",
       "        0.94736842, 0.96491228, 0.96491228, 0.96491228, 0.96491228,\n",
       "        1.        , 0.92982456, 0.96491228, 0.92982456, 0.94736842,\n",
       "        0.92982456, 0.98245614, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.92982456, 0.96491228,\n",
       "        0.94736842, 0.96491228, 0.94736842, 1.        , 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.9122807 , 0.94736842, 0.92982456,\n",
       "        0.98245614, 0.98245614, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.92982456, 0.98245614, 0.96491228, 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.94736842, 0.92982456, 0.98245614,\n",
       "        0.96491228, 0.98245614, 0.98245614, 0.98245614, 0.96491228,\n",
       "        0.98245614, 0.98245614, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.96491228, 0.98245614, 0.96491228,\n",
       "        0.98245614, 0.98245614, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.94736842, 0.98245614, 0.96491228, 0.98245614, 0.94736842,\n",
       "        0.98245614, 1.        , 0.98245614, 0.94736842, 0.98245614,\n",
       "        0.96491228, 0.98245614, 0.96491228, 0.98245614, 0.98245614,\n",
       "        0.98245614, 1.        , 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.92982456, 0.98245614, 0.9122807 ,\n",
       "        0.98245614, 0.96491228, 0.96491228, 0.94736842, 0.98245614,\n",
       "        0.96491228, 0.98245614, 1.        , 0.98245614, 0.92982456,\n",
       "        0.96491228, 0.98245614, 0.96491228, 0.94736842, 0.96491228,\n",
       "        0.98245614, 0.98245614, 0.9122807 , 0.98245614, 0.94736842,\n",
       "        0.98245614, 0.92982456, 0.98245614, 0.96491228, 0.98245614,\n",
       "        0.92982456, 0.98245614, 0.98245614, 0.98245614, 0.96491228,\n",
       "        0.98245614, 0.98245614, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.96491228, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.98245614, 0.98245614, 1.        ,\n",
       "        0.98245614, 0.94736842, 0.96491228, 0.96491228, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.96491228, 0.98245614, 0.92982456,\n",
       "        0.98245614, 0.98245614, 0.96491228, 0.94736842, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.98245614, 0.96491228,\n",
       "        0.98245614, 0.98245614, 0.98245614, 0.96491228, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.96491228, 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.96491228, 0.98245614, 1.        , 0.98245614,\n",
       "        0.94736842, 0.98245614, 0.96491228, 0.98245614, 0.94736842,\n",
       "        0.98245614, 0.94736842, 0.98245614, 0.92982456, 0.98245614,\n",
       "        0.96491228, 0.98245614, 0.98245614, 0.98245614, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.9122807 , 0.98245614, 0.96491228, 0.96491228, 0.96491228,\n",
       "        0.96491228, 0.98245614, 0.96491228, 0.94736842, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.94736842, 0.96491228, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.98245614, 1.        , 0.98245614,\n",
       "        0.96491228, 0.98245614, 0.98245614, 0.98245614, 0.94736842,\n",
       "        0.98245614, 0.96491228, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.94736842, 0.98245614, 0.92982456, 0.98245614, 0.94736842,\n",
       "        0.98245614, 1.        , 0.98245614, 0.96491228, 0.98245614,\n",
       "        0.92982456, 0.98245614, 0.9122807 , 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.96491228, 0.96491228, 0.96491228, 0.98245614,\n",
       "        0.98245614, 0.98245614, 1.        , 0.96491228, 0.92982456,\n",
       "        0.98245614, 0.94736842, 1.        , 1.        , 0.98245614,\n",
       "        0.98245614, 0.98245614, 1.        , 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.92982456, 0.98245614, 0.96491228, 0.98245614,\n",
       "        0.96491228, 0.98245614, 0.98245614, 0.98245614, 0.96491228,\n",
       "        0.98245614, 0.96491228, 0.98245614, 0.94736842, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.96491228, 0.98245614, 0.98245614,\n",
       "        0.98245614, 1.        , 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.96491228, 0.98245614, 0.98245614, 0.98245614, 0.9122807 ]),\n",
       " 'split5_test_score': array([0.87719298, 0.94736842, 0.89473684, 0.96491228, 0.89473684,\n",
       "        0.96491228, 0.87719298, 0.94736842, 0.89473684, 0.92982456,\n",
       "        0.89473684, 0.98245614, 0.89473684, 0.94736842, 0.87719298,\n",
       "        0.94736842, 0.87719298, 0.92982456, 0.87719298, 0.94736842,\n",
       "        0.87719298, 0.92982456, 0.87719298, 0.96491228, 0.87719298,\n",
       "        0.9122807 , 0.87719298, 0.96491228, 0.87719298, 0.92982456,\n",
       "        0.92982456, 0.96491228, 0.92982456, 0.94736842, 0.9122807 ,\n",
       "        0.92982456, 0.92982456, 0.96491228, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.98245614, 0.92982456, 0.98245614, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.89473684, 0.94736842, 0.89473684, 0.96491228, 0.89473684,\n",
       "        0.98245614, 0.89473684, 0.94736842, 0.89473684, 0.94736842,\n",
       "        0.89473684, 0.96491228, 0.89473684, 0.94736842, 0.87719298,\n",
       "        0.94736842, 0.87719298, 1.        , 0.87719298, 0.94736842,\n",
       "        0.89473684, 0.92982456, 0.87719298, 0.96491228, 0.89473684,\n",
       "        0.96491228, 0.87719298, 0.94736842, 0.87719298, 0.98245614,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.98245614, 0.92982456, 1.        ,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.9122807 , 0.92982456, 0.89473684,\n",
       "        0.9122807 , 0.96491228, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.94736842, 0.89473684, 0.9122807 , 0.87719298, 0.92982456,\n",
       "        0.89473684, 0.96491228, 0.89473684, 0.92982456, 0.87719298,\n",
       "        0.98245614, 0.87719298, 0.94736842, 0.87719298, 0.94736842,\n",
       "        0.89473684, 0.92982456, 0.87719298, 0.96491228, 0.89473684,\n",
       "        0.92982456, 0.87719298, 0.92982456, 0.89473684, 0.92982456,\n",
       "        0.92982456, 0.96491228, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.94736842, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.98245614, 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.92982456, 0.92982456, 0.94736842,\n",
       "        0.87719298, 0.96491228, 0.87719298, 0.94736842, 0.89473684,\n",
       "        0.96491228, 0.89473684, 0.92982456, 0.89473684, 0.96491228,\n",
       "        0.87719298, 0.96491228, 0.89473684, 0.94736842, 0.87719298,\n",
       "        0.98245614, 0.87719298, 0.96491228, 0.87719298, 0.89473684,\n",
       "        0.89473684, 0.96491228, 0.87719298, 0.94736842, 0.89473684,\n",
       "        0.94736842, 0.89473684, 0.96491228, 0.87719298, 0.96491228,\n",
       "        0.92982456, 0.96491228, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.92982456, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.92982456, 0.92982456, 1.        , 0.92982456, 0.96491228,\n",
       "        0.9122807 , 0.96491228, 0.87719298, 0.96491228, 0.89473684,\n",
       "        0.94736842, 0.89473684, 0.9122807 , 0.89473684, 0.98245614,\n",
       "        0.87719298, 0.98245614, 0.89473684, 0.96491228, 0.87719298,\n",
       "        0.94736842, 0.87719298, 0.94736842, 0.87719298, 0.92982456,\n",
       "        0.87719298, 0.94736842, 0.87719298, 0.92982456, 0.87719298,\n",
       "        0.96491228, 0.89473684, 0.92982456, 0.89473684, 0.94736842,\n",
       "        0.92982456, 0.96491228, 0.92982456, 1.        , 0.92982456,\n",
       "        0.94736842, 0.9122807 , 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.98245614, 0.92982456, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.92982456, 0.98245614, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.92982456, 0.96491228, 0.94736842,\n",
       "        0.92982456, 0.98245614, 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.96491228, 0.94736842, 1.        , 0.96491228, 0.96491228,\n",
       "        0.94736842, 0.9122807 , 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.96491228, 0.92982456, 0.94736842, 0.96491228, 0.96491228,\n",
       "        0.98245614, 0.98245614, 0.94736842, 1.        , 0.94736842,\n",
       "        0.92982456, 0.98245614, 0.94736842, 0.94736842, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.9122807 , 0.96491228, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.94736842, 0.94736842,\n",
       "        0.98245614, 0.92982456, 0.96491228, 0.96491228, 0.98245614,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.96491228, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.96491228, 0.96491228, 0.96491228,\n",
       "        0.94736842, 0.94736842, 0.92982456, 1.        , 0.94736842,\n",
       "        0.98245614, 0.92982456, 0.94736842, 0.96491228, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.9122807 , 0.96491228, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.94736842, 0.96491228,\n",
       "        0.98245614, 0.96491228, 0.94736842, 0.96491228, 0.92982456,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.98245614, 0.94736842, 0.92982456, 0.96491228, 0.94736842,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.96491228, 0.92982456,\n",
       "        0.9122807 , 0.94736842, 0.98245614, 0.96491228, 0.98245614,\n",
       "        0.94736842, 0.92982456, 0.92982456, 0.96491228, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.96491228, 0.96491228, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.96491228, 0.98245614, 0.98245614, 0.92982456,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.94736842, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.98245614, 0.96491228, 0.94736842,\n",
       "        0.92982456, 0.9122807 , 0.94736842, 0.94736842, 0.92982456,\n",
       "        0.98245614, 0.94736842, 0.96491228, 0.96491228, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.94736842, 0.92982456, 0.96491228, 0.96491228, 0.94736842,\n",
       "        0.94736842, 0.92982456, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.92982456, 0.94736842, 0.94736842, 0.96491228, 0.92982456,\n",
       "        0.94736842, 0.96491228, 0.94736842, 0.96491228, 0.96491228,\n",
       "        0.96491228, 0.98245614, 0.92982456, 0.94736842, 0.98245614,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.94736842, 0.96491228,\n",
       "        0.94736842, 0.96491228, 0.94736842, 0.96491228, 0.98245614,\n",
       "        0.94736842, 0.94736842, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.9122807 , 0.92982456, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.94736842, 0.9122807 , 0.92982456, 0.94736842, 0.94736842,\n",
       "        0.98245614, 0.92982456, 0.9122807 , 0.96491228, 0.92982456,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.92982456, 0.96491228, 0.94736842, 0.98245614,\n",
       "        0.94736842, 0.94736842, 0.98245614, 0.92982456, 0.94736842,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.94736842, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.98245614, 0.96491228, 0.92982456, 0.96491228, 0.96491228]),\n",
       " 'split6_test_score': array([0.87719298, 0.92982456, 0.89473684, 0.94736842, 0.9122807 ,\n",
       "        0.89473684, 0.85964912, 0.92982456, 0.87719298, 0.87719298,\n",
       "        0.9122807 , 0.9122807 , 0.89473684, 0.92982456, 0.89473684,\n",
       "        0.9122807 , 0.89473684, 0.89473684, 0.87719298, 0.96491228,\n",
       "        0.9122807 , 0.94736842, 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.92982456, 0.89473684, 0.92982456, 0.89473684, 0.96491228,\n",
       "        0.9122807 , 0.9122807 , 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.9122807 , 0.9122807 , 0.9122807 , 0.9122807 , 0.9122807 ,\n",
       "        0.94736842, 0.9122807 , 0.94736842, 0.9122807 , 0.96491228,\n",
       "        0.87719298, 0.92982456, 0.89473684, 0.89473684, 0.89473684,\n",
       "        0.94736842, 0.89473684, 0.92982456, 0.87719298, 0.94736842,\n",
       "        0.9122807 , 0.9122807 , 0.87719298, 0.96491228, 0.9122807 ,\n",
       "        0.89473684, 0.85964912, 0.92982456, 0.87719298, 0.89473684,\n",
       "        0.9122807 , 0.96491228, 0.9122807 , 0.87719298, 0.9122807 ,\n",
       "        0.89473684, 0.89473684, 0.89473684, 0.87719298, 0.89473684,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.9122807 , 0.92982456, 0.87719298,\n",
       "        0.9122807 , 0.89473684, 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.94736842, 0.9122807 , 0.9122807 , 0.9122807 , 0.92982456,\n",
       "        0.87719298, 0.89473684, 0.85964912, 0.94736842, 0.84210526,\n",
       "        0.92982456, 0.87719298, 0.94736842, 0.85964912, 0.92982456,\n",
       "        0.87719298, 0.9122807 , 0.87719298, 0.9122807 , 0.89473684,\n",
       "        0.9122807 , 0.89473684, 0.92982456, 0.87719298, 0.89473684,\n",
       "        0.9122807 , 0.87719298, 0.9122807 , 0.85964912, 0.9122807 ,\n",
       "        0.94736842, 0.89473684, 0.92982456, 0.87719298, 0.92982456,\n",
       "        0.9122807 , 0.96491228, 0.92982456, 0.89473684, 0.9122807 ,\n",
       "        0.9122807 , 0.9122807 , 0.92982456, 0.92982456, 0.94736842,\n",
       "        0.9122807 , 0.94736842, 0.9122807 , 0.96491228, 0.9122807 ,\n",
       "        0.9122807 , 0.9122807 , 0.9122807 , 0.9122807 , 0.92982456,\n",
       "        0.87719298, 0.92982456, 0.9122807 , 0.98245614, 0.89473684,\n",
       "        0.92982456, 0.89473684, 0.9122807 , 0.85964912, 0.96491228,\n",
       "        0.89473684, 0.9122807 , 0.87719298, 0.92982456, 0.87719298,\n",
       "        0.9122807 , 0.89473684, 0.9122807 , 0.87719298, 0.92982456,\n",
       "        0.9122807 , 0.9122807 , 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.9122807 , 0.89473684, 0.94736842, 0.87719298, 0.96491228,\n",
       "        0.9122807 , 0.98245614, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.9122807 , 0.92982456, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.9122807 , 0.84210526, 0.9122807 , 0.85964912, 0.9122807 ,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.9122807 , 0.94736842,\n",
       "        0.84210526, 0.9122807 , 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.85964912, 0.87719298, 0.98245614, 0.89473684, 0.94736842,\n",
       "        0.9122807 , 0.87719298, 0.89473684, 0.84210526, 0.9122807 ,\n",
       "        0.9122807 , 0.89473684, 0.94736842, 0.89473684, 0.94736842,\n",
       "        0.9122807 , 0.85964912, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.96491228, 0.89473684, 0.94736842, 0.89473684, 0.92982456,\n",
       "        0.9122807 , 0.94736842, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.92982456, 0.9122807 , 0.98245614, 0.9122807 , 0.94736842,\n",
       "        0.9122807 , 0.94736842, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.9122807 , 0.9122807 , 0.92982456, 0.9122807 , 0.85964912,\n",
       "        0.94736842, 0.87719298, 0.92982456, 0.89473684, 0.94736842,\n",
       "        0.9122807 , 0.92982456, 0.96491228, 0.94736842, 0.94736842,\n",
       "        0.9122807 , 0.89473684, 0.92982456, 0.89473684, 0.94736842,\n",
       "        0.89473684, 0.92982456, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.98245614, 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.96491228, 0.92982456, 0.92982456,\n",
       "        0.94736842, 0.9122807 , 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.98245614, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.9122807 , 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.89473684, 0.92982456, 0.92982456, 0.92982456, 0.9122807 ,\n",
       "        0.89473684, 0.92982456, 0.94736842, 0.89473684, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.9122807 , 0.92982456, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.87719298, 0.94736842, 0.96491228, 0.94736842, 0.89473684,\n",
       "        0.94736842, 0.89473684, 0.94736842, 0.89473684, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.87719298, 0.94736842, 0.9122807 , 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.89473684, 0.94736842, 0.94736842,\n",
       "        0.92982456, 0.89473684, 0.9122807 , 0.89473684, 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.85964912, 0.92982456, 0.92982456,\n",
       "        0.9122807 , 0.96491228, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.96491228, 0.92982456, 0.94736842,\n",
       "        0.94736842, 0.96491228, 0.94736842, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.94736842, 0.96491228, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.9122807 , 0.94736842,\n",
       "        0.89473684, 0.94736842, 0.94736842, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.9122807 , 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.96491228, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.92982456, 0.9122807 , 0.94736842,\n",
       "        0.87719298, 0.92982456, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.92982456, 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.96491228, 0.94736842, 0.9122807 ,\n",
       "        0.92982456, 0.92982456, 0.92982456, 0.87719298, 0.94736842,\n",
       "        0.89473684, 0.94736842, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.94736842, 0.9122807 , 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.96491228, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.9122807 , 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.92982456, 0.94736842, 0.94736842,\n",
       "        0.9122807 , 0.92982456, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.92982456, 0.9122807 ,\n",
       "        0.92982456, 0.98245614, 0.92982456, 0.92982456, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.98245614, 0.94736842, 0.9122807 , 0.92982456, 0.89473684,\n",
       "        0.94736842, 0.87719298, 0.94736842, 0.9122807 , 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.94736842, 0.9122807 , 0.94736842, 0.89473684, 0.94736842,\n",
       "        0.9122807 , 0.94736842, 0.94736842, 0.94736842, 0.92982456]),\n",
       " 'split7_test_score': array([0.94736842, 0.92982456, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.87719298, 0.94736842, 0.9122807 , 0.94736842, 0.92982456,\n",
       "        0.92982456, 0.96491228, 0.94736842, 0.98245614, 0.94736842,\n",
       "        0.9122807 , 0.94736842, 0.94736842, 0.94736842, 0.92982456,\n",
       "        0.92982456, 0.96491228, 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.92982456, 0.9122807 , 0.96491228, 0.9122807 , 0.94736842,\n",
       "        0.92982456, 0.98245614, 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.96491228, 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.94736842, 0.94736842, 0.96491228, 0.94736842, 0.96491228,\n",
       "        0.94736842, 0.9122807 , 0.94736842, 0.89473684, 0.94736842,\n",
       "        0.98245614, 0.94736842, 0.94736842, 0.94736842, 0.96491228,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.92982456, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.98245614, 0.94736842, 0.98245614,\n",
       "        0.94736842, 0.96491228, 0.94736842, 0.98245614, 0.92982456,\n",
       "        0.9122807 , 0.9122807 , 1.        , 0.9122807 , 0.96491228,\n",
       "        0.92982456, 0.94736842, 0.96491228, 0.96491228, 0.96491228,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.92982456, 1.        ,\n",
       "        0.89473684, 0.89473684, 0.92982456, 0.98245614, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.98245614, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.98245614, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.9122807 , 0.94736842, 0.98245614,\n",
       "        0.94736842, 0.96491228, 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.92982456, 0.9122807 , 0.98245614, 0.9122807 , 0.92982456,\n",
       "        0.92982456, 0.96491228, 0.96491228, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.96491228, 1.        , 0.96491228, 0.94736842,\n",
       "        0.89473684, 0.94736842, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.98245614, 0.9122807 , 0.98245614, 0.9122807 , 0.94736842,\n",
       "        0.94736842, 1.        , 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.92982456, 0.89473684, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.96491228, 0.92982456, 0.96491228, 0.94736842, 0.92982456,\n",
       "        0.92982456, 0.96491228, 0.92982456, 0.98245614, 0.92982456,\n",
       "        0.9122807 , 0.9122807 , 1.        , 0.9122807 , 0.98245614,\n",
       "        0.96491228, 0.98245614, 0.96491228, 0.92982456, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.89473684, 0.98245614, 0.9122807 , 0.92982456, 0.92982456,\n",
       "        0.9122807 , 0.94736842, 0.98245614, 0.9122807 , 0.96491228,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.96491228, 0.94736842, 1.        , 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.89473684, 0.94736842,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.98245614, 0.92982456, 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.92982456, 0.96491228, 0.92982456, 0.98245614, 0.92982456,\n",
       "        0.94736842, 0.92982456, 1.        , 0.92982456, 0.94736842,\n",
       "        0.92982456, 0.96491228, 0.9122807 , 0.94736842, 0.89473684,\n",
       "        0.98245614, 0.92982456, 0.96491228, 0.92982456, 0.94736842,\n",
       "        0.96491228, 0.94736842, 0.96491228, 0.98245614, 0.9122807 ,\n",
       "        0.94736842, 0.94736842, 0.98245614, 0.96491228, 0.9122807 ,\n",
       "        0.94736842, 0.96491228, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.9122807 , 0.94736842, 0.98245614, 0.96491228, 0.98245614,\n",
       "        0.96491228, 0.98245614, 0.96491228, 0.96491228, 0.94736842,\n",
       "        0.9122807 , 0.96491228, 0.98245614, 0.96491228, 0.92982456,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.96491228, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.96491228, 0.96491228,\n",
       "        0.96491228, 0.94736842, 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.96491228, 0.98245614,\n",
       "        0.96491228, 0.98245614, 0.94736842, 0.9122807 , 0.92982456,\n",
       "        0.96491228, 0.94736842, 0.96491228, 0.96491228, 0.94736842,\n",
       "        0.96491228, 0.98245614, 0.92982456, 0.96491228, 0.92982456,\n",
       "        0.92982456, 0.96491228, 0.98245614, 0.96491228, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.96491228, 0.94736842, 0.94736842,\n",
       "        0.92982456, 0.96491228, 0.94736842, 0.96491228, 0.9122807 ,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.9122807 , 0.96491228,\n",
       "        0.94736842, 0.96491228, 0.96491228, 0.96491228, 0.92982456,\n",
       "        0.96491228, 0.98245614, 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.92982456, 0.96491228, 0.98245614, 0.96491228, 0.92982456,\n",
       "        0.94736842, 0.98245614, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.98245614, 0.94736842, 0.94736842, 0.96491228, 0.96491228,\n",
       "        0.96491228, 0.9122807 , 0.94736842, 0.9122807 , 0.96491228,\n",
       "        0.89473684, 0.94736842, 0.96491228, 0.96491228, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.9122807 , 0.96491228, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.96491228, 1.        , 0.96491228, 1.        , 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.96491228, 0.98245614,\n",
       "        0.96491228, 0.94736842, 0.96491228, 0.87719298, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.98245614, 0.96491228, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.96491228, 0.96491228, 0.92982456,\n",
       "        0.96491228, 0.96491228, 0.98245614, 0.96491228, 0.94736842,\n",
       "        0.96491228, 0.96491228, 0.94736842, 0.89473684, 0.96491228,\n",
       "        0.98245614, 0.96491228, 0.96491228, 0.96491228, 0.98245614,\n",
       "        0.94736842, 0.96491228, 0.94736842, 0.94736842, 0.96491228,\n",
       "        0.98245614, 0.96491228, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.96491228, 0.98245614, 0.96491228, 1.        , 0.96491228,\n",
       "        0.94736842, 0.96491228, 0.89473684, 0.96491228, 0.98245614,\n",
       "        0.96491228, 0.98245614, 0.96491228, 0.9122807 , 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.92982456, 0.96491228, 0.98245614,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.98245614, 0.92982456,\n",
       "        0.92982456, 0.94736842, 0.96491228, 0.96491228, 0.94736842,\n",
       "        0.96491228, 0.9122807 , 0.94736842, 0.89473684, 0.94736842,\n",
       "        0.89473684, 0.96491228, 0.96491228, 0.96491228, 0.96491228,\n",
       "        0.96491228, 0.94736842, 0.92982456, 0.98245614, 0.94736842,\n",
       "        1.        , 0.96491228, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.96491228, 0.98245614, 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.98245614, 0.96491228, 0.94736842,\n",
       "        0.96491228, 0.98245614, 0.96491228, 0.98245614, 0.96491228,\n",
       "        0.94736842, 0.96491228, 0.92982456, 0.96491228, 0.89473684]),\n",
       " 'split8_test_score': array([0.9122807 , 0.96491228, 0.92982456, 0.87719298, 0.92982456,\n",
       "        0.96491228, 0.92982456, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.9122807 , 0.92982456, 0.9122807 , 0.96491228, 0.9122807 ,\n",
       "        0.92982456, 0.9122807 , 0.92982456, 0.9122807 , 0.96491228,\n",
       "        0.9122807 , 0.92982456, 0.9122807 , 0.9122807 , 0.9122807 ,\n",
       "        0.92982456, 0.9122807 , 0.94736842, 0.9122807 , 0.98245614,\n",
       "        0.98245614, 0.9122807 , 0.98245614, 0.9122807 , 0.98245614,\n",
       "        0.96491228, 0.98245614, 0.92982456, 0.98245614, 0.9122807 ,\n",
       "        0.9122807 , 0.98245614, 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.85964912, 0.9122807 , 0.96491228, 0.9122807 , 0.9122807 ,\n",
       "        0.92982456, 0.98245614, 0.9122807 , 0.87719298, 0.92982456,\n",
       "        0.94736842, 0.92982456, 0.87719298, 0.92982456, 0.96491228,\n",
       "        0.9122807 , 0.94736842, 0.9122807 , 0.89473684, 0.9122807 ,\n",
       "        0.92982456, 0.9122807 , 0.89473684, 0.9122807 , 0.98245614,\n",
       "        0.9122807 , 0.92982456, 0.9122807 , 0.9122807 , 0.9122807 ,\n",
       "        0.89473684, 0.9122807 , 0.94736842, 0.9122807 , 0.96491228,\n",
       "        0.98245614, 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.96491228, 0.98245614, 0.92982456, 0.98245614, 0.92982456,\n",
       "        0.9122807 , 0.96491228, 0.9122807 , 0.9122807 , 0.9122807 ,\n",
       "        0.94736842, 0.9122807 , 0.92982456, 0.9122807 , 0.94736842,\n",
       "        0.92982456, 0.96491228, 0.9122807 , 0.92982456, 0.92982456,\n",
       "        0.89473684, 0.92982456, 0.94736842, 0.9122807 , 0.94736842,\n",
       "        0.9122807 , 0.9122807 , 0.9122807 , 0.92982456, 0.9122807 ,\n",
       "        0.94736842, 0.9122807 , 0.92982456, 0.9122807 , 0.87719298,\n",
       "        0.9122807 , 0.92982456, 0.9122807 , 0.96491228, 0.9122807 ,\n",
       "        0.9122807 , 0.9122807 , 0.92982456, 0.9122807 , 0.94736842,\n",
       "        0.98245614, 0.92982456, 0.98245614, 0.9122807 , 0.94736842,\n",
       "        0.94736842, 0.98245614, 0.9122807 , 0.94736842, 0.96491228,\n",
       "        0.9122807 , 0.94736842, 0.9122807 , 0.96491228, 0.9122807 ,\n",
       "        0.9122807 , 0.9122807 , 0.92982456, 0.9122807 , 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.9122807 , 0.9122807 , 0.92982456,\n",
       "        0.92982456, 0.9122807 , 0.96491228, 0.92982456, 0.98245614,\n",
       "        0.9122807 , 0.89473684, 0.9122807 , 0.9122807 , 0.9122807 ,\n",
       "        0.89473684, 0.9122807 , 0.96491228, 0.9122807 , 0.92982456,\n",
       "        0.9122807 , 0.89473684, 0.9122807 , 0.96491228, 0.9122807 ,\n",
       "        0.92982456, 0.9122807 , 0.98245614, 0.9122807 , 0.94736842,\n",
       "        0.94736842, 0.87719298, 0.98245614, 0.94736842, 0.98245614,\n",
       "        0.98245614, 0.94736842, 0.96491228, 0.98245614, 0.87719298,\n",
       "        0.9122807 , 0.94736842, 0.9122807 , 0.9122807 , 0.9122807 ,\n",
       "        0.94736842, 0.9122807 , 0.94736842, 0.9122807 , 0.94736842,\n",
       "        0.9122807 , 0.9122807 , 0.92982456, 0.94736842, 0.9122807 ,\n",
       "        0.92982456, 0.92982456, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.9122807 , 0.94736842, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.94736842, 0.9122807 , 0.9122807 , 0.9122807 , 0.94736842,\n",
       "        0.9122807 , 0.9122807 , 0.9122807 , 0.98245614, 0.9122807 ,\n",
       "        0.87719298, 0.9122807 , 0.9122807 , 0.9122807 , 0.96491228,\n",
       "        0.94736842, 0.9122807 , 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.9122807 , 0.98245614, 0.89473684, 0.98245614, 0.94736842,\n",
       "        0.9122807 , 0.96491228, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.9122807 , 0.9122807 , 0.92982456, 0.9122807 , 0.89473684,\n",
       "        0.96491228, 0.94736842, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.9122807 , 0.94736842, 0.89473684, 0.94736842, 0.9122807 ,\n",
       "        0.96491228, 0.89473684, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.87719298, 0.94736842, 0.9122807 , 0.94736842, 0.92982456,\n",
       "        0.92982456, 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.96491228, 0.92982456, 0.94736842, 0.96491228,\n",
       "        0.94736842, 0.87719298, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.94736842, 0.94736842, 0.9122807 ,\n",
       "        0.94736842, 0.98245614, 0.94736842, 0.9122807 , 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.94736842, 0.96491228,\n",
       "        0.92982456, 0.9122807 , 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.9122807 , 0.96491228, 0.92982456, 0.94736842, 0.9122807 ,\n",
       "        0.94736842, 0.96491228, 0.94736842, 0.92982456, 0.92982456,\n",
       "        0.92982456, 0.96491228, 0.9122807 , 0.94736842, 0.89473684,\n",
       "        0.96491228, 0.96491228, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.89473684, 0.96491228, 0.92982456, 0.94736842, 0.96491228,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.9122807 , 0.94736842,\n",
       "        0.87719298, 0.94736842, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.96491228, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.87719298, 0.94736842, 0.92982456,\n",
       "        0.94736842, 0.89473684, 0.94736842, 0.94736842, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.96491228, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.9122807 , 0.96491228, 0.92982456, 0.94736842,\n",
       "        0.8245614 , 0.94736842, 0.92982456, 0.94736842, 0.96491228,\n",
       "        0.96491228, 0.87719298, 0.96491228, 0.98245614, 0.96491228,\n",
       "        0.89473684, 0.94736842, 0.92982456, 0.94736842, 0.96491228,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.9122807 , 0.94736842, 0.9122807 ,\n",
       "        0.94736842, 0.89473684, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.92982456, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.89473684, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.85964912, 0.96491228, 0.89473684, 0.94736842, 0.94736842,\n",
       "        0.96491228, 0.92982456, 0.92982456, 0.98245614, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.92982456, 0.87719298, 0.96491228, 0.92982456, 0.96491228,\n",
       "        0.96491228, 0.94736842, 0.94736842, 0.94736842, 0.96491228,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.9122807 , 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.92982456, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.85964912, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.92982456, 0.94736842, 0.9122807 , 0.96491228,\n",
       "        0.92982456, 0.96491228, 0.98245614, 0.94736842, 0.92982456,\n",
       "        0.96491228, 0.9122807 , 0.96491228, 0.87719298, 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.96491228, 0.94736842, 0.9122807 ,\n",
       "        0.96491228, 0.94736842, 0.94736842, 0.94736842, 0.96491228,\n",
       "        0.92982456, 0.94736842, 0.9122807 , 0.94736842, 0.96491228,\n",
       "        0.94736842, 0.89473684, 0.94736842, 0.87719298, 0.94736842,\n",
       "        0.92982456, 0.94736842, 0.84210526, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.87719298, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.9122807 , 0.94736842, 0.94736842, 0.94736842, 0.85964912]),\n",
       " 'split9_test_score': array([0.94642857, 1.        , 0.94642857, 0.80357143, 0.94642857,\n",
       "        0.96428571, 0.96428571, 0.96428571, 0.98214286, 0.92857143,\n",
       "        0.92857143, 0.89285714, 0.94642857, 0.91071429, 0.96428571,\n",
       "        0.92857143, 0.94642857, 0.91071429, 0.96428571, 0.96428571,\n",
       "        0.96428571, 0.96428571, 0.92857143, 0.91071429, 0.89285714,\n",
       "        0.91071429, 0.96428571, 0.91071429, 0.96428571, 1.        ,\n",
       "        1.        , 0.94642857, 1.        , 0.91071429, 0.94642857,\n",
       "        0.96428571, 0.94642857, 0.94642857, 0.96428571, 0.96428571,\n",
       "        0.96428571, 0.92857143, 1.        , 0.98214286, 0.96428571,\n",
       "        0.875     , 1.        , 0.98214286, 1.        , 0.96428571,\n",
       "        0.94642857, 0.98214286, 0.94642857, 0.92857143, 0.94642857,\n",
       "        0.92857143, 0.98214286, 0.91071429, 0.96428571, 0.94642857,\n",
       "        0.89285714, 0.94642857, 0.92857143, 0.91071429, 0.96428571,\n",
       "        0.92857143, 0.96428571, 0.92857143, 0.96428571, 0.96428571,\n",
       "        0.96428571, 0.92857143, 0.91071429, 0.94642857, 0.96428571,\n",
       "        0.91071429, 0.94642857, 0.91071429, 0.96428571, 0.92857143,\n",
       "        1.        , 0.91071429, 0.96428571, 0.89285714, 1.        ,\n",
       "        0.96428571, 0.94642857, 0.94642857, 1.        , 0.98214286,\n",
       "        0.96428571, 0.96428571, 0.96428571, 0.98214286, 0.96428571,\n",
       "        0.98214286, 1.        , 0.96428571, 1.        , 0.92857143,\n",
       "        0.94642857, 0.89285714, 0.94642857, 0.875     , 0.98214286,\n",
       "        0.98214286, 0.96428571, 0.91071429, 0.98214286, 0.94642857,\n",
       "        0.92857143, 0.92857143, 0.96428571, 0.92857143, 0.96428571,\n",
       "        0.91071429, 0.94642857, 0.92857143, 0.96428571, 0.89285714,\n",
       "        0.96428571, 0.94642857, 0.89285714, 0.96428571, 0.96428571,\n",
       "        0.85714286, 0.94642857, 0.89285714, 0.94642857, 0.96428571,\n",
       "        1.        , 0.98214286, 1.        , 0.94642857, 0.92857143,\n",
       "        0.92857143, 1.        , 0.94642857, 0.92857143, 0.91071429,\n",
       "        0.96428571, 1.        , 0.96428571, 0.92857143, 0.96428571,\n",
       "        0.91071429, 0.96428571, 0.94642857, 0.96428571, 0.96428571,\n",
       "        0.92857143, 0.94642857, 0.94642857, 0.91071429, 0.94642857,\n",
       "        0.96428571, 0.98214286, 0.92857143, 0.92857143, 0.96428571,\n",
       "        0.91071429, 0.91071429, 0.94642857, 0.89285714, 0.91071429,\n",
       "        0.94642857, 0.96428571, 0.94642857, 0.94642857, 0.85714286,\n",
       "        0.96428571, 0.92857143, 0.96428571, 0.89285714, 0.96428571,\n",
       "        0.92857143, 0.94642857, 1.        , 0.94642857, 0.96428571,\n",
       "        1.        , 0.94642857, 0.96428571, 0.96428571, 0.92857143,\n",
       "        0.94642857, 0.96428571, 0.94642857, 1.        , 0.92857143,\n",
       "        0.96428571, 0.89285714, 1.        , 0.98214286, 1.        ,\n",
       "        0.98214286, 1.        , 0.92857143, 0.96428571, 0.98214286,\n",
       "        0.98214286, 0.91071429, 0.92857143, 0.96428571, 0.98214286,\n",
       "        0.83928571, 0.91071429, 0.98214286, 0.96428571, 0.94642857,\n",
       "        0.94642857, 0.91071429, 0.94642857, 0.91071429, 0.89285714,\n",
       "        0.89285714, 0.92857143, 0.92857143, 0.96428571, 0.94642857,\n",
       "        0.875     , 0.98214286, 0.875     , 0.94642857, 0.96428571,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.96428571,\n",
       "        1.        , 1.        , 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.91071429, 0.96428571, 0.94642857, 1.        , 0.96428571,\n",
       "        1.        , 0.94642857, 1.        , 0.98214286, 1.        ,\n",
       "        0.89285714, 1.        , 0.96428571, 0.96428571, 0.96428571,\n",
       "        0.91071429, 0.98214286, 0.89285714, 0.94642857, 0.94642857,\n",
       "        0.89285714, 0.94642857, 0.94642857, 0.94642857, 0.94642857,\n",
       "        0.92857143, 0.96428571, 0.89285714, 0.98214286, 0.89285714,\n",
       "        0.96428571, 0.96428571, 0.83928571, 0.92857143, 0.92857143,\n",
       "        0.94642857, 0.89285714, 0.91071429, 0.96428571, 0.89285714,\n",
       "        0.92857143, 0.96428571, 0.98214286, 0.92857143, 0.91071429,\n",
       "        0.94642857, 0.94642857, 0.94642857, 0.91071429, 0.94642857,\n",
       "        0.98214286, 0.94642857, 0.96428571, 0.94642857, 0.94642857,\n",
       "        0.94642857, 0.96428571, 0.98214286, 0.94642857, 0.98214286,\n",
       "        0.94642857, 0.98214286, 0.89285714, 0.98214286, 0.96428571,\n",
       "        0.92857143, 0.92857143, 0.96428571, 0.96428571, 0.96428571,\n",
       "        0.96428571, 0.94642857, 0.94642857, 0.94642857, 0.94642857,\n",
       "        0.89285714, 0.94642857, 0.92857143, 0.91071429, 0.96428571,\n",
       "        0.92857143, 0.94642857, 0.96428571, 0.94642857, 0.875     ,\n",
       "        0.92857143, 0.96428571, 0.91071429, 0.89285714, 0.91071429,\n",
       "        0.96428571, 0.96428571, 0.83928571, 0.92857143, 0.89285714,\n",
       "        0.94642857, 0.91071429, 0.94642857, 0.96428571, 0.94642857,\n",
       "        0.96428571, 0.94642857, 0.91071429, 0.94642857, 0.94642857,\n",
       "        0.94642857, 0.92857143, 0.98214286, 0.94642857, 0.98214286,\n",
       "        0.91071429, 0.98214286, 0.94642857, 0.98214286, 0.89285714,\n",
       "        0.94642857, 0.96428571, 0.92857143, 0.92857143, 0.94642857,\n",
       "        0.92857143, 0.92857143, 0.94642857, 0.92857143, 0.85714286,\n",
       "        0.91071429, 1.        , 0.91071429, 0.89285714, 0.94642857,\n",
       "        0.94642857, 0.92857143, 0.98214286, 0.92857143, 1.        ,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.89285714, 0.94642857,\n",
       "        0.92857143, 0.92857143, 0.91071429, 0.92857143, 0.96428571,\n",
       "        0.94642857, 0.96428571, 0.94642857, 0.98214286, 0.94642857,\n",
       "        0.92857143, 0.94642857, 0.92857143, 0.94642857, 0.875     ,\n",
       "        0.96428571, 0.94642857, 0.98214286, 0.94642857, 0.98214286,\n",
       "        0.89285714, 0.96428571, 0.96428571, 0.96428571, 0.96428571,\n",
       "        0.92857143, 0.96428571, 0.89285714, 0.94642857, 0.89285714,\n",
       "        0.89285714, 0.94642857, 0.92857143, 0.94642857, 0.91071429,\n",
       "        0.89285714, 0.92857143, 0.92857143, 0.98214286, 0.94642857,\n",
       "        0.91071429, 0.96428571, 0.89285714, 0.94642857, 0.94642857,\n",
       "        0.875     , 0.92857143, 0.92857143, 0.89285714, 0.91071429,\n",
       "        0.96428571, 0.92857143, 0.92857143, 0.94642857, 0.94642857,\n",
       "        0.94642857, 0.94642857, 0.94642857, 0.91071429, 0.94642857,\n",
       "        0.91071429, 0.94642857, 0.91071429, 0.94642857, 0.92857143,\n",
       "        0.98214286, 0.92857143, 0.96428571, 0.91071429, 0.98214286,\n",
       "        0.89285714, 0.94642857, 0.91071429, 0.96428571, 0.98214286,\n",
       "        0.92857143, 0.89285714, 0.91071429, 0.94642857, 0.96428571,\n",
       "        0.89285714, 0.94642857, 0.96428571, 0.92857143, 0.94642857,\n",
       "        0.92857143, 0.875     , 0.92857143, 0.91071429, 0.94642857,\n",
       "        0.89285714, 0.94642857, 0.91071429, 0.94642857, 0.94642857,\n",
       "        0.91071429, 0.98214286, 0.89285714, 0.96428571, 0.875     ,\n",
       "        0.94642857, 0.94642857, 0.91071429, 0.94642857, 0.96428571,\n",
       "        0.94642857, 0.94642857, 0.94642857, 0.91071429, 0.94642857,\n",
       "        0.91071429, 0.94642857, 0.89285714, 0.94642857, 0.94642857,\n",
       "        0.96428571, 0.92857143, 0.94642857, 0.94642857, 0.96428571,\n",
       "        0.98214286, 0.96428571, 0.94642857, 0.96428571, 0.92857143]),\n",
       " 'mean_test_score': array([0.90692356, 0.94736842, 0.92095865, 0.90140977, 0.91218672,\n",
       "        0.92800752, 0.90870927, 0.92274436, 0.9087406 , 0.92794486,\n",
       "        0.91917293, 0.93314536, 0.91744987, 0.94721178, 0.91572682,\n",
       "        0.91215539, 0.90867794, 0.92265038, 0.9052005 , 0.93853383,\n",
       "        0.90870927, 0.94555138, 0.90162907, 0.92791353, 0.90156642,\n",
       "        0.92089599, 0.89642857, 0.94194862, 0.89818296, 0.94736842,\n",
       "        0.9245614 , 0.93499373, 0.92807018, 0.93142231, 0.91920426,\n",
       "        0.94555138, 0.92271303, 0.94552005, 0.92274436, 0.94555138,\n",
       "        0.92449875, 0.94548872, 0.92807018, 0.94733709, 0.92625313,\n",
       "        0.93135965, 0.93157895, 0.94382832, 0.93157895, 0.94028822,\n",
       "        0.91920426, 0.94031955, 0.92095865, 0.91741855, 0.91920426,\n",
       "        0.94022556, 0.91049499, 0.92791353, 0.91221805, 0.94727444,\n",
       "        0.90858396, 0.92973058, 0.91390977, 0.92265038, 0.91046366,\n",
       "        0.93320802, 0.90695489, 0.94899749, 0.90169173, 0.93853383,\n",
       "        0.91046366, 0.9226817 , 0.90159774, 0.93499373, 0.9052005 ,\n",
       "        0.91036967, 0.89639724, 0.92615915, 0.89993734, 0.93671679,\n",
       "        0.92807018, 0.92966792, 0.92274436, 0.92963659, 0.92631579,\n",
       "        0.94379699, 0.92095865, 0.93850251, 0.92807018, 0.94909148,\n",
       "        0.92274436, 0.92800752, 0.92625313, 0.92628446, 0.92449875,\n",
       "        0.94558271, 0.92807018, 0.93677945, 0.92982456, 0.93496241,\n",
       "        0.91218672, 0.93489975, 0.91218672, 0.93837719, 0.91926692,\n",
       "        0.92277569, 0.91572682, 0.92615915, 0.90172306, 0.92973058,\n",
       "        0.91566416, 0.92619048, 0.92098997, 0.94022556, 0.91397243,\n",
       "        0.93493108, 0.90867794, 0.92443609, 0.9052005 , 0.93139098,\n",
       "        0.9052005 , 0.92973058, 0.89805764, 0.9297619 , 0.9052005 ,\n",
       "        0.91378446, 0.90341479, 0.92963659, 0.89990602, 0.93853383,\n",
       "        0.92631579, 0.93505639, 0.93157895, 0.92973058, 0.91566416,\n",
       "        0.93320802, 0.92982456, 0.94727444, 0.91917293, 0.92791353,\n",
       "        0.92274436, 0.94385965, 0.92800752, 0.92443609, 0.92449875,\n",
       "        0.93142231, 0.92449875, 0.93674812, 0.92449875, 0.93151629,\n",
       "        0.91566416, 0.93323935, 0.90867794, 0.92265038, 0.91920426,\n",
       "        0.93327068, 0.91049499, 0.93496241, 0.90338346, 0.93853383,\n",
       "        0.91212406, 0.92265038, 0.9139411 , 0.93314536, 0.91212406,\n",
       "        0.93674812, 0.90344612, 0.93850251, 0.89990602, 0.91203008,\n",
       "        0.9052005 , 0.92969925, 0.90695489, 0.94191729, 0.91221805,\n",
       "        0.91741855, 0.89464286, 0.96491228, 0.89990602, 0.94906015,\n",
       "        0.9245614 , 0.94727444, 0.92625313, 0.93327068, 0.91917293,\n",
       "        0.94025689, 0.92098997, 0.93499373, 0.92807018, 0.93671679,\n",
       "        0.92098997, 0.92788221, 0.92631579, 0.93681078, 0.92982456,\n",
       "        0.93330201, 0.92982456, 0.93847118, 0.92449875, 0.93856516,\n",
       "        0.91575815, 0.92966792, 0.91917293, 0.94555138, 0.9210213 ,\n",
       "        0.92778822, 0.90335213, 0.95260025, 0.91397243, 0.92797619,\n",
       "        0.91569549, 0.93317669, 0.92095865, 0.92265038, 0.91209273,\n",
       "        0.93489975, 0.90338346, 0.93320802, 0.90344612, 0.93850251,\n",
       "        0.90328947, 0.92453008, 0.90153509, 0.93148496, 0.90870927,\n",
       "        0.93320802, 0.90513784, 0.92619048, 0.89987469, 0.93853383,\n",
       "        0.92105263, 0.94561404, 0.91741855, 0.95426065, 0.91566416,\n",
       "        0.92615915, 0.92098997, 0.92973058, 0.92631579, 0.93853383,\n",
       "        0.92807018, 0.93499373, 0.92631579, 0.94031955, 0.92631579,\n",
       "        0.92612782, 0.92982456, 0.93502506, 0.92449875, 0.9297619 ,\n",
       "        0.94019424, 0.93505639, 0.93139098, 0.93674812, 0.93323935,\n",
       "        0.93665414, 0.93148496, 0.94376566, 0.93499373, 0.93148496,\n",
       "        0.92969925, 0.94028822, 0.92788221, 0.94909148, 0.92788221,\n",
       "        0.92274436, 0.93151629, 0.92954261, 0.93320802, 0.94548872,\n",
       "        0.93674812, 0.93489975, 0.93493108, 0.96309524, 0.93489975,\n",
       "        0.92969925, 0.93677945, 0.94909148, 0.92969925, 0.92966792,\n",
       "        0.93850251, 0.93674812, 0.93674812, 0.93668546, 0.93323935,\n",
       "        0.92803885, 0.94201128, 0.95256892, 0.93850251, 0.93148496,\n",
       "        0.94201128, 0.94906015, 0.94207393, 0.93850251, 0.94909148,\n",
       "        0.92446742, 0.94031955, 0.92612782, 0.93856516, 0.94204261,\n",
       "        0.93145363, 0.93847118, 0.93677945, 0.93677945, 0.93327068,\n",
       "        0.94730576, 0.93323935, 0.95253759, 0.93499373, 0.93674812,\n",
       "        0.92788221, 0.94201128, 0.92092732, 0.93493108, 0.9297619 ,\n",
       "        0.93847118, 0.94201128, 0.93853383, 0.93323935, 0.92258772,\n",
       "        0.93671679, 0.94204261, 0.93493108, 0.94016291, 0.93493108,\n",
       "        0.92625313, 0.94204261, 0.92603383, 0.93320802, 0.92788221,\n",
       "        0.93148496, 0.93843985, 0.93850251, 0.92800752, 0.93850251,\n",
       "        0.93151629, 0.93850251, 0.92791353, 0.94025689, 0.93674812,\n",
       "        0.94025689, 0.93145363, 0.94207393, 0.93323935, 0.94382832,\n",
       "        0.94370301, 0.93505639, 0.93674812, 0.94207393, 0.93665414,\n",
       "        0.93674812, 0.9297619 , 0.92969925, 0.93671679, 0.92446742,\n",
       "        0.94022556, 0.92969925, 0.92973058, 0.93145363, 0.93659148,\n",
       "        0.92966792, 0.93333333, 0.94019424, 0.92437343, 0.93850251,\n",
       "        0.91569549, 0.92619048, 0.93505639, 0.93496241, 0.95438596,\n",
       "        0.94022556, 0.93145363, 0.93847118, 0.93840852, 0.93499373,\n",
       "        0.92794486, 0.93320802, 0.93668546, 0.93320802, 0.92800752,\n",
       "        0.93499373, 0.93853383, 0.94201128, 0.94733709, 0.93323935,\n",
       "        0.93847118, 0.94376566, 0.93671679, 0.93323935, 0.92785088,\n",
       "        0.93502506, 0.93323935, 0.93681078, 0.93850251, 0.94733709,\n",
       "        0.93489975, 0.94204261, 0.95256892, 0.94204261, 0.94204261,\n",
       "        0.92443609, 0.92800752, 0.93314536, 0.94201128, 0.93139098,\n",
       "        0.91735589, 0.93499373, 0.94548872, 0.93499373, 0.93317669,\n",
       "        0.92788221, 0.93847118, 0.93145363, 0.93856516, 0.94552005,\n",
       "        0.93843985, 0.93853383, 0.94191729, 0.93499373, 0.94201128,\n",
       "        0.92609649, 0.92969925, 0.93145363, 0.92788221, 0.93668546,\n",
       "        0.93677945, 0.93671679, 0.92443609, 0.93499373, 0.94201128,\n",
       "        0.93850251, 0.94025689, 0.93323935, 0.94194862, 0.94025689,\n",
       "        0.93317669, 0.93674812, 0.91738722, 0.94201128, 0.93320802,\n",
       "        0.94031955, 0.94373434, 0.94379699, 0.93493108, 0.93505639,\n",
       "        0.92612782, 0.94552005, 0.92615915, 0.93677945, 0.95084586,\n",
       "        0.93145363, 0.92437343, 0.92615915, 0.92797619, 0.94028822,\n",
       "        0.91911028, 0.93148496, 0.95081454, 0.92619048, 0.92973058,\n",
       "        0.93496241, 0.92258772, 0.93145363, 0.92265038, 0.93148496,\n",
       "        0.92261905, 0.93323935, 0.93493108, 0.93674812, 0.94025689,\n",
       "        0.94019424, 0.93154762, 0.93314536, 0.95256892, 0.93662281,\n",
       "        0.94902882, 0.93850251, 0.93668546, 0.93323935, 0.93327068,\n",
       "        0.93499373, 0.92973058, 0.93499373, 0.91738722, 0.93850251,\n",
       "        0.93843985, 0.93323935, 0.92261905, 0.93850251, 0.93148496,\n",
       "        0.93677945, 0.94022556, 0.93323935, 0.92973058, 0.93502506,\n",
       "        0.94031955, 0.94555138, 0.92797619, 0.94379699, 0.91741855]),\n",
       " 'std_test_score': array([0.03502457, 0.03234928, 0.03155061, 0.04457527, 0.02817322,\n",
       "        0.0371131 , 0.04346226, 0.03931928, 0.04131208, 0.02981956,\n",
       "        0.03153939, 0.03332491, 0.042945  , 0.02503572, 0.03977513,\n",
       "        0.04502376, 0.03818233, 0.01798936, 0.04157498, 0.04311437,\n",
       "        0.03299579, 0.02873263, 0.03336841, 0.02005524, 0.02511656,\n",
       "        0.03068745, 0.03868514, 0.02741042, 0.03896543, 0.03038686,\n",
       "        0.05207306, 0.02937095, 0.0492792 , 0.0254861 , 0.04649327,\n",
       "        0.02275472, 0.04377275, 0.02535793, 0.04779848, 0.02873263,\n",
       "        0.03325793, 0.02544353, 0.03879534, 0.02931917, 0.03302908,\n",
       "        0.04286201, 0.03879534, 0.02910468, 0.03879534, 0.02499686,\n",
       "        0.0305037 , 0.03522542, 0.02101009, 0.02829112, 0.02947742,\n",
       "        0.02631012, 0.04600168, 0.02153532, 0.03671107, 0.01359234,\n",
       "        0.03224529, 0.02346909, 0.03542644, 0.03163461, 0.03708347,\n",
       "        0.02580389, 0.04071742, 0.04041251, 0.04302504, 0.04451925,\n",
       "        0.03538459, 0.02953807, 0.03053843, 0.03421169, 0.03336023,\n",
       "        0.03459715, 0.03620666, 0.04142359, 0.0383713 , 0.0325682 ,\n",
       "        0.0492792 , 0.03603363, 0.04649279, 0.03439621, 0.05072573,\n",
       "        0.04207434, 0.0465299 , 0.03349408, 0.0492792 , 0.04255333,\n",
       "        0.03430249, 0.0362743 , 0.03302908, 0.04420426, 0.03325793,\n",
       "        0.02764218, 0.03879534, 0.01944669, 0.03843667, 0.03335523,\n",
       "        0.0292453 , 0.02861488, 0.03835182, 0.03108238, 0.04087069,\n",
       "        0.04161959, 0.03482407, 0.02337355, 0.0444646 , 0.0322995 ,\n",
       "        0.03210589, 0.03662258, 0.02846357, 0.02111854, 0.0317219 ,\n",
       "        0.03435163, 0.03818233, 0.02721376, 0.04157498, 0.03391021,\n",
       "        0.03769207, 0.03323876, 0.02809932, 0.03995161, 0.0360219 ,\n",
       "        0.03128719, 0.03246075, 0.03255741, 0.03500574, 0.02505396,\n",
       "        0.04949732, 0.03420101, 0.05051291, 0.02711956, 0.0390288 ,\n",
       "        0.0302005 , 0.05084694, 0.02481237, 0.04449327, 0.02886372,\n",
       "        0.03430249, 0.03663265, 0.03363261, 0.03846138, 0.03325793,\n",
       "        0.02666644, 0.03325793, 0.03250982, 0.03325793, 0.03363915,\n",
       "        0.02908807, 0.04280014, 0.02902269, 0.03864205, 0.0305037 ,\n",
       "        0.02448143, 0.03873733, 0.02726219, 0.03934298, 0.03778819,\n",
       "        0.03039049, 0.03163461, 0.0379122 , 0.02224805, 0.03138693,\n",
       "        0.03936175, 0.03690519, 0.01957459, 0.04069771, 0.03276313,\n",
       "        0.0360219 , 0.04575033, 0.03502865, 0.02642603, 0.03318845,\n",
       "        0.03923106, 0.0383088 , 0.02602175, 0.03411516, 0.02532452,\n",
       "        0.05087719, 0.03138468, 0.04944879, 0.03741105, 0.04585593,\n",
       "        0.03345303, 0.04163575, 0.02719444, 0.0492792 , 0.02853871,\n",
       "        0.03434413, 0.04551862, 0.03907203, 0.0352223 , 0.03843667,\n",
       "        0.02038434, 0.03922926, 0.04790984, 0.03325793, 0.02626369,\n",
       "        0.0448878 , 0.02365869, 0.03054792, 0.02275472, 0.03858664,\n",
       "        0.05039036, 0.03697465, 0.02831014, 0.03791017, 0.0410634 ,\n",
       "        0.02567029, 0.03129574, 0.03155061, 0.03531258, 0.03339082,\n",
       "        0.03446964, 0.03152544, 0.03986479, 0.03853709, 0.02382932,\n",
       "        0.02767766, 0.04367767, 0.02646976, 0.03456014, 0.03653699,\n",
       "        0.03828951, 0.02946355, 0.01887577, 0.02928392, 0.02505396,\n",
       "        0.04915413, 0.0379937 , 0.03923106, 0.03867798, 0.04206515,\n",
       "        0.0173232 , 0.045181  , 0.05318291, 0.04949732, 0.03938352,\n",
       "        0.03879534, 0.03681184, 0.03907203, 0.04012698, 0.03985199,\n",
       "        0.03595252, 0.03843667, 0.03037658, 0.03325793, 0.03837989,\n",
       "        0.02642835, 0.03420101, 0.02441077, 0.03154886, 0.03398118,\n",
       "        0.02769742, 0.03079242, 0.02694063, 0.03681184, 0.03079242,\n",
       "        0.03843851, 0.03249223, 0.03478708, 0.02532183, 0.03478708,\n",
       "        0.04160141, 0.03542184, 0.04562429, 0.03664658, 0.02420364,\n",
       "        0.0295333 , 0.03069082, 0.03776591, 0.01992136, 0.03069082,\n",
       "        0.02219461, 0.03774453, 0.02650948, 0.04000792, 0.02613137,\n",
       "        0.04017851, 0.04237425, 0.02847207, 0.029699  , 0.03742927,\n",
       "        0.04667774, 0.03042251, 0.01574179, 0.02743189, 0.04733792,\n",
       "        0.02606337, 0.03545286, 0.04369025, 0.02382932, 0.02650948,\n",
       "        0.0489858 , 0.04379454, 0.03038485, 0.04452513, 0.03138748,\n",
       "        0.0327835 , 0.02636021, 0.02498806, 0.02238947, 0.03657908,\n",
       "        0.02598015, 0.0311456 , 0.02362548, 0.03681184, 0.0224248 ,\n",
       "        0.03104692, 0.0351186 , 0.04589427, 0.0325103 , 0.04071472,\n",
       "        0.03166461, 0.02485442, 0.03778819, 0.03659771, 0.03377549,\n",
       "        0.02959755, 0.02219717, 0.02847262, 0.04331858, 0.03055822,\n",
       "        0.0373993 , 0.02602665, 0.03968408, 0.03828951, 0.03566088,\n",
       "        0.04107143, 0.03454628, 0.02508773, 0.04105081, 0.02743189,\n",
       "        0.034542  , 0.02959094, 0.03555301, 0.02377159, 0.0224248 ,\n",
       "        0.02623363, 0.03548845, 0.04369025, 0.02450926, 0.02910468,\n",
       "        0.02476644, 0.04507231, 0.03694151, 0.04369025, 0.02656294,\n",
       "        0.02501972, 0.03322155, 0.02829112, 0.0262934 , 0.04961014,\n",
       "        0.03530211, 0.03038918, 0.03504183, 0.03634539, 0.03830653,\n",
       "        0.04231881, 0.03746343, 0.02126565, 0.02110711, 0.02250065,\n",
       "        0.03899702, 0.03117481, 0.04368522, 0.03046144, 0.03253901,\n",
       "        0.02855411, 0.05051866, 0.02860027, 0.03189223, 0.03596602,\n",
       "        0.03633329, 0.03579685, 0.0286439 , 0.03828951, 0.04796606,\n",
       "        0.03844771, 0.03612246, 0.02485442, 0.0350565 , 0.03742927,\n",
       "        0.02118091, 0.02454959, 0.03615131, 0.04490572, 0.03020351,\n",
       "        0.04296624, 0.03742927, 0.04448932, 0.02959094, 0.02713852,\n",
       "        0.03356484, 0.02718352, 0.02484247, 0.02718352, 0.0111072 ,\n",
       "        0.03331566, 0.02646225, 0.03042824, 0.02485442, 0.03006115,\n",
       "        0.0478497 , 0.03329988, 0.02989319, 0.02830363, 0.03411884,\n",
       "        0.04050978, 0.02636021, 0.02878416, 0.02957116, 0.02535793,\n",
       "        0.04030319, 0.03346879, 0.02398375, 0.03509981, 0.03423096,\n",
       "        0.03608839, 0.02718143, 0.03183081, 0.03297008, 0.029699  ,\n",
       "        0.02950649, 0.02743903, 0.04709529, 0.03329988, 0.02485442,\n",
       "        0.02743189, 0.03251996, 0.03981987, 0.03438361, 0.02848365,\n",
       "        0.02591595, 0.04013608, 0.02364242, 0.02358357, 0.03747705,\n",
       "        0.04379454, 0.04066199, 0.02690306, 0.03055822, 0.04507231,\n",
       "        0.02935442, 0.02411363, 0.02815231, 0.04307611, 0.03015095,\n",
       "        0.03084871, 0.04314314, 0.03408661, 0.03628854, 0.02951394,\n",
       "        0.03361393, 0.03543953, 0.02803946, 0.04060788, 0.0174519 ,\n",
       "        0.03768764, 0.0277748 , 0.04185557, 0.03783715, 0.0387581 ,\n",
       "        0.04324239, 0.0311456 , 0.03857229, 0.03055771, 0.0224395 ,\n",
       "        0.02642835, 0.02871878, 0.03042824, 0.01574179, 0.03001183,\n",
       "        0.02537093, 0.02382932, 0.03266042, 0.03211862, 0.03741105,\n",
       "        0.0323624 , 0.04769073, 0.04226125, 0.02837594, 0.02508773,\n",
       "        0.03077687, 0.04351332, 0.04464324, 0.02743189, 0.03543953,\n",
       "        0.04307611, 0.03258169, 0.04207487, 0.03590943, 0.04296624,\n",
       "        0.03856241, 0.02531584, 0.03078171, 0.02690306, 0.03420134]),\n",
       " 'rank_test_score': array([469,  20, 406, 490, 446, 314, 460, 382, 459, 324, 420, 241, 422,\n",
       "         29, 432, 449, 464, 390, 470, 102, 460,  32, 486, 325, 488, 411,\n",
       "        498,  71, 496,  21, 363, 184, 306, 268, 413,  32, 388,  37, 382,\n",
       "         32, 366,  42, 306,  22, 345, 273, 245,  44, 245,  80, 413,  75,\n",
       "        408, 423, 413,  90, 454, 325, 444,  26, 466, 285, 442, 390, 456,\n",
       "        230, 467,  19, 485, 102, 456, 389, 487, 184, 470, 458, 499, 353,\n",
       "        491, 157, 306, 299, 382, 303, 338,  46, 406, 110, 306,  12, 382,\n",
       "        314, 345, 344, 366,  31, 306, 138, 274, 195, 446, 204, 446, 135,\n",
       "        412, 381, 431, 353, 484, 285, 435, 349, 402,  89, 439, 197, 464,\n",
       "        375, 470, 270, 470, 285, 497, 279, 470, 443, 479, 303, 492, 102,\n",
       "        338, 171, 245, 285, 435, 229, 274,  27, 417, 325, 382,  43, 314,\n",
       "        375, 366, 268, 366, 145, 366, 249, 435, 215, 463, 390, 413, 211,\n",
       "        454, 193, 480, 102, 450, 390, 441, 241, 450, 145, 477, 110, 492,\n",
       "        453, 470, 292, 467,  73, 444, 423, 500,   1, 492,  16, 363,  27,\n",
       "        345, 211, 417,  83, 402, 179, 306, 157, 402, 329, 342, 137, 274,\n",
       "        210, 274, 130, 366,  98, 430, 302, 417,  32, 401, 337, 482,   5,\n",
       "        439, 321, 434, 238, 408, 390, 452, 206, 480, 230, 477, 110, 483,\n",
       "        365, 489, 252, 460, 230, 476, 352, 495, 102, 400,  30, 423,   4,\n",
       "        435, 353, 402, 283, 338, 102, 306, 179, 342,  75, 338, 358, 274,\n",
       "        176, 366, 279,  94, 171, 270, 145, 215, 167, 252,  49, 184, 252,\n",
       "        292,  80, 329,  12, 329, 387, 250, 305, 230,  40, 145, 206, 197,\n",
       "          2, 206, 292, 138,  12, 292, 299, 110, 145, 145, 163, 215, 313,\n",
       "         66,   6, 110, 252,  66,  16,  53, 110,  15, 373,  75, 358,  99,\n",
       "         56, 260, 125, 138, 142, 211,  25, 215,   9, 184, 145, 329,  62,\n",
       "        410, 197, 279, 125,  62, 102, 215, 398, 157,  56, 197,  97, 197,\n",
       "        345,  56, 362, 230, 329, 252, 132, 110, 319, 110, 250, 110, 325,\n",
       "         83, 145,  85, 260,  53, 215,  45,  52, 171, 145,  53, 167, 145,\n",
       "        279, 292, 157, 373,  90, 292, 283, 260, 170, 299, 209,  94, 379,\n",
       "        110, 433, 349, 171, 195,   3,  90, 260, 125, 134, 179, 323, 230,\n",
       "        163, 230, 314, 184, 102,  66,  22, 215, 125,  49, 162, 215, 336,\n",
       "        176, 215, 136, 110,  22, 204,  60,   6,  60,  56, 375, 314, 241,\n",
       "         62, 270, 429, 179,  40, 179, 238, 329, 125, 260,  99,  37, 131,\n",
       "        101,  73, 184,  66, 361, 292, 260, 329, 163, 138, 157, 375, 184,\n",
       "         62, 110,  85, 215,  71,  85, 238, 145, 428,  66, 230,  75,  51,\n",
       "         47, 197, 171, 358,  39, 353, 142,  10, 260, 379, 353, 321,  80,\n",
       "        421, 252,  11, 349, 285, 193, 398, 260, 390, 252, 396, 215, 197,\n",
       "        145,  85,  94, 248, 241,   6, 169,  18, 110, 166, 215, 211, 184,\n",
       "        285, 184, 427, 110, 132, 228, 396, 110, 252, 142,  90, 215, 285,\n",
       "        176,  75,  32, 320,  47, 423])}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9649122807017543,\n",
       " DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=30, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=3, min_samples_split=8,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='random'),\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 30,\n",
       "  'min_samples_leaf': 3,\n",
       "  'min_samples_split': 8,\n",
       "  'splitter': 'random'})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_, grid_search.best_estimator_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "C = [0.1, 0.5, 1, 5]\n",
    "kernel = ['linear', 'rbf']\n",
    "param_grid = [{'C': C, 'kernel':kernel}]\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid=[{'C': [0.1, 0.5, 1, 5], 'kernel': ['linear', 'rbf']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(data_x,data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.09796696, 0.00360222, 0.3153995 , 0.00278249, 0.80087147,\n",
       "        0.00259252, 1.66075163, 0.00219364]),\n",
       " 'std_fit_time': array([0.07088525, 0.00047417, 0.11098342, 0.00074404, 0.31816041,\n",
       "        0.00049093, 0.33717674, 0.00039985]),\n",
       " 'mean_score_time': array([0.00039849, 0.00059204, 0.00058975, 0.00039897, 0.00059891,\n",
       "        0.00060334, 0.00019932, 0.000596  ]),\n",
       " 'std_score_time': array([0.00048805, 0.00048349, 0.00048166, 0.00048864, 0.00048901,\n",
       "        0.00049271, 0.00039864, 0.00048665]),\n",
       " 'param_C': masked_array(data=[0.1, 0.1, 0.5, 0.5, 1, 1, 5, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_kernel': masked_array(data=['linear', 'rbf', 'linear', 'rbf', 'linear', 'rbf',\n",
       "                    'linear', 'rbf'],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.1, 'kernel': 'linear'},\n",
       "  {'C': 0.1, 'kernel': 'rbf'},\n",
       "  {'C': 0.5, 'kernel': 'linear'},\n",
       "  {'C': 0.5, 'kernel': 'rbf'},\n",
       "  {'C': 1, 'kernel': 'linear'},\n",
       "  {'C': 1, 'kernel': 'rbf'},\n",
       "  {'C': 5, 'kernel': 'linear'},\n",
       "  {'C': 5, 'kernel': 'rbf'}],\n",
       " 'split0_test_score': array([0.93859649, 0.83333333, 0.93859649, 0.85087719, 0.94736842,\n",
       "        0.85087719, 0.94736842, 0.88596491]),\n",
       " 'split1_test_score': array([0.94736842, 0.86842105, 0.93859649, 0.89473684, 0.92982456,\n",
       "        0.89473684, 0.93859649, 0.92982456]),\n",
       " 'split2_test_score': array([0.98245614, 0.90350877, 0.96491228, 0.92982456, 0.97368421,\n",
       "        0.92982456, 0.97368421, 0.93859649]),\n",
       " 'split3_test_score': array([0.92105263, 0.93859649, 0.92105263, 0.94736842, 0.92105263,\n",
       "        0.94736842, 0.92982456, 0.94736842]),\n",
       " 'split4_test_score': array([0.95575221, 0.91150442, 0.95575221, 0.92920354, 0.95575221,\n",
       "        0.9380531 , 0.96460177, 0.95575221]),\n",
       " 'mean_test_score': array([0.94904518, 0.89107281, 0.94378202, 0.91040211, 0.94553641,\n",
       "        0.91217202, 0.95081509, 0.93150132]),\n",
       " 'std_test_score': array([0.02028224, 0.03654464, 0.01523271, 0.03430991, 0.01868869,\n",
       "        0.03544367, 0.01621632, 0.02435811]),\n",
       " 'rank_test_score': array([2, 8, 4, 7, 3, 6, 1, 5])}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9508150908244062,\n",
       " SVC(C=5, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "     max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "     tol=0.001, verbose=False),\n",
       " {'C': 5, 'kernel': 'linear'})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_, grid_search.best_estimator_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=5,kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=cross_val_score(clf,data_x,data_y,scoring='accuracy',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=[0.1, 0.5, 1, 5]\n",
    "kernel = ['linear', 'rbf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96491228 0.92982456 0.92982456 0.94736842 0.98245614 0.96491228\n",
      " 0.92982456 0.9122807  0.96491228 0.94642857]\n",
      "[0.84210526 0.8245614  0.84210526 0.89473684 0.89473684 0.92982456\n",
      " 0.94736842 0.94736842 0.9122807  0.89285714]\n",
      "[0.98245614 0.92982456 0.92982456 0.94736842 0.98245614 0.96491228\n",
      " 0.92982456 0.94736842 0.96491228 0.94642857]\n",
      "[0.87719298 0.80701754 0.87719298 0.9122807  0.92982456 0.92982456\n",
      " 0.94736842 0.94736842 0.92982456 0.91071429]\n",
      "[0.98245614 0.92982456 0.92982456 0.94736842 0.96491228 0.98245614\n",
      " 0.92982456 0.94736842 0.96491228 0.96428571]\n",
      "[0.89473684 0.84210526 0.89473684 0.92982456 0.92982456 0.92982456\n",
      " 0.94736842 0.92982456 0.92982456 0.91071429]\n",
      "[0.98245614 0.92982456 0.92982456 0.94736842 0.96491228 0.98245614\n",
      " 0.92982456 0.94736842 0.96491228 0.96428571]\n",
      "[0.87719298 0.87719298 0.89473684 0.94736842 0.94736842 0.89473684\n",
      " 0.96491228 0.92982456 0.96491228 0.94642857]\n"
     ]
    }
   ],
   "source": [
    "for i in C:\n",
    "    for j in kernel:\n",
    "        clf=SVC(C=i,kernel=j)\n",
    "        scores=cross_val_score(clf,data_x,data_y,cv=10,scoring='accuracy')\n",
    "        print(scores)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(criterion='entropy',n_estimators=120,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=time.time()\n",
    "scores=cross_val_score(clf,data_x,data_y,scoring='accuracy',cv=10)\n",
    "t2=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.095310688018799"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "n_estimators = [10, 20, 35, 50, 80, 100, 120, 150, 200]\n",
    "criterion=['gini','entropy']\n",
    "param_grid = [{'n_estimators': n_estimators,'criterion':criterion}]\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False, random_state=0,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid=[{'criterion': ['gini', 'entropy'],\n",
       "                          'n_estimators': [10, 20, 35, 50, 80, 100, 120, 150,\n",
       "                                           200]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(data_x,data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01540952, 0.0291858 , 0.05117037, 0.07428877, 0.11390514,\n",
       "        0.14105287, 0.17378356, 0.22354844, 0.29020236, 0.01607182,\n",
       "        0.03277018, 0.05559833, 0.07920351, 0.13080564, 0.16293001,\n",
       "        0.18930371, 0.23604403, 0.31246543]),\n",
       " 'std_fit_time': array([0.00081868, 0.00136326, 0.00182547, 0.00347854, 0.00257266,\n",
       "        0.00326353, 0.00694092, 0.01330525, 0.01563853, 0.00061473,\n",
       "        0.00120793, 0.0017106 , 0.00226823, 0.00515625, 0.00537245,\n",
       "        0.00430923, 0.00299776, 0.00450495]),\n",
       " 'mean_score_time': array([0.00112092, 0.0020149 , 0.00252821, 0.00355241, 0.00521541,\n",
       "        0.00630357, 0.00883589, 0.01038859, 0.01278577, 0.00099509,\n",
       "        0.00143368, 0.00249505, 0.00337286, 0.00550096, 0.0065593 ,\n",
       "        0.00728214, 0.00898712, 0.01188889]),\n",
       " 'std_score_time': array([2.92701495e-04, 6.31090884e-04, 4.73570488e-04, 5.69997948e-04,\n",
       "        6.98029137e-04, 4.43070630e-04, 2.62966498e-03, 1.51123674e-03,\n",
       "        9.83588381e-04, 1.16425678e-05, 4.41344360e-04, 4.99551207e-04,\n",
       "        5.64634875e-04, 7.94713079e-04, 6.98952829e-04, 4.53001565e-04,\n",
       "        6.66174465e-04, 6.60997683e-04]),\n",
       " 'param_criterion': masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[10, 20, 35, 50, 80, 100, 120, 150, 200, 10, 20, 35, 50,\n",
       "                    80, 100, 120, 150, 200],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'criterion': 'gini', 'n_estimators': 10},\n",
       "  {'criterion': 'gini', 'n_estimators': 20},\n",
       "  {'criterion': 'gini', 'n_estimators': 35},\n",
       "  {'criterion': 'gini', 'n_estimators': 50},\n",
       "  {'criterion': 'gini', 'n_estimators': 80},\n",
       "  {'criterion': 'gini', 'n_estimators': 100},\n",
       "  {'criterion': 'gini', 'n_estimators': 120},\n",
       "  {'criterion': 'gini', 'n_estimators': 150},\n",
       "  {'criterion': 'gini', 'n_estimators': 200},\n",
       "  {'criterion': 'entropy', 'n_estimators': 10},\n",
       "  {'criterion': 'entropy', 'n_estimators': 20},\n",
       "  {'criterion': 'entropy', 'n_estimators': 35},\n",
       "  {'criterion': 'entropy', 'n_estimators': 50},\n",
       "  {'criterion': 'entropy', 'n_estimators': 80},\n",
       "  {'criterion': 'entropy', 'n_estimators': 100},\n",
       "  {'criterion': 'entropy', 'n_estimators': 120},\n",
       "  {'criterion': 'entropy', 'n_estimators': 150},\n",
       "  {'criterion': 'entropy', 'n_estimators': 200}],\n",
       " 'split0_test_score': array([0.98245614, 0.98245614, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.98245614, 0.98245614, 0.96491228,\n",
       "        0.98245614, 0.98245614, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.98245614]),\n",
       " 'split1_test_score': array([0.85964912, 0.89473684, 0.89473684, 0.89473684, 0.9122807 ,\n",
       "        0.89473684, 0.89473684, 0.89473684, 0.89473684, 0.9122807 ,\n",
       "        0.9122807 , 0.89473684, 0.9122807 , 0.89473684, 0.89473684,\n",
       "        0.92982456, 0.89473684, 0.89473684]),\n",
       " 'split2_test_score': array([0.9122807 , 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.92982456, 0.94736842, 0.92982456,\n",
       "        0.92982456, 0.92982456, 0.9122807 , 0.92982456, 0.92982456,\n",
       "        0.94736842, 0.94736842, 0.92982456]),\n",
       " 'split3_test_score': array([0.94736842, 0.96491228, 0.96491228, 0.96491228, 0.94736842,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.96491228, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842]),\n",
       " 'split4_test_score': array([0.98245614, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98245614, 0.98245614, 0.98245614,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        ]),\n",
       " 'split5_test_score': array([0.98245614, 0.98245614, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.98245614, 0.98245614, 0.96491228,\n",
       "        0.96491228, 0.98245614, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.98245614]),\n",
       " 'split6_test_score': array([0.9122807 , 0.9122807 , 0.92982456, 0.92982456, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.96491228, 0.94736842,\n",
       "        0.96491228, 0.96491228, 0.96491228, 0.96491228, 0.98245614,\n",
       "        0.98245614, 0.96491228, 0.96491228]),\n",
       " 'split7_test_score': array([1.        , 0.98245614, 0.98245614, 0.96491228, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.98245614, 0.98245614, 0.98245614,\n",
       "        0.98245614, 0.98245614, 0.98245614]),\n",
       " 'split8_test_score': array([0.92982456, 0.92982456, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.94736842, 0.96491228,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842]),\n",
       " 'split9_test_score': array([0.96428571, 1.        , 0.98214286, 0.98214286, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.98214286,\n",
       "        0.98214286, 0.98214286, 1.        , 0.98214286, 0.96428571,\n",
       "        0.98214286, 0.98214286, 0.98214286]),\n",
       " 'mean_test_score': array([0.94730576, 0.95964912, 0.96137218, 0.95961779, 0.96491228,\n",
       "        0.96491228, 0.96491228, 0.96140351, 0.96491228, 0.95786341,\n",
       "        0.96137218, 0.96137218, 0.96315789, 0.96137218, 0.96134085,\n",
       "        0.96838972, 0.96312657, 0.96137218]),\n",
       " 'std_test_score': array([0.04149029, 0.03513155, 0.03016188, 0.02938471, 0.02717883,\n",
       "        0.03038686, 0.03038686, 0.0301836 , 0.02828862, 0.02243302,\n",
       "        0.02575868, 0.03016188, 0.03083929, 0.03016188, 0.0301769 ,\n",
       "        0.0218924 , 0.0287532 , 0.03016188]),\n",
       " 'rank_test_score': array([18, 15,  9, 16,  5,  2,  2,  8,  2, 17,  9,  9,  6,  9, 14,  1,  7,\n",
       "         9])}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9683897243107771,\n",
       " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='entropy', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=120,\n",
       "                        n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                        warm_start=False),\n",
       " {'criterion': 'entropy', 'n_estimators': 120})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_, grid_search.best_estimator_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  xgboost  as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=xgb.XGBClassifier(learning_rate=0.01,\n",
    "                      n_estimators=30,           # 树的个数-10棵树建立xgboost\n",
    "                      max_depth=4,               # 树的深度\n",
    "                      min_child_weight = 1,      # 叶子节点最小权重\n",
    "                      gamma=0.1,                  # 惩罚项中叶子结点个数前的参数\n",
    "                      subsample=1,               # 所有样本建立决策树\n",
    "                      scale_pos_weight=1,        # 解决样本个数不平衡的问题\n",
    "\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0.1, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=30, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(data_x,data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=cross_val_score(clf,data_x,data_y,cv=10,scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94736842, 0.85964912, 0.9122807 , 0.89473684, 0.98245614,\n",
       "       0.94736842, 0.92982456, 0.96491228, 0.96491228, 0.98214286])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9385651629072683"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
